# k8s
***

## 基本概念
1. master 节点:负责整个集群控制,管理,有以下关键组件

  * kube-apiserver:k8s所有资源crud的接口,是控制集群的入口
  * kube-controller-manager:k8s里所有资源的自动化控制中心
  * kube-seheduler:负责pod调度的进程
2. node 计算节点,负责工作负载,有以下关键组件

  * kubelet:负责pod创建,启停.与master协作管理集群
  * kube-proxy:实现k8s service的通信和lb
  * docker engine:负责本机容器创建和管理


## 部署篇:kubeadm

准备开始

* 一台兼容的 Linux 主机。Kubernetes 项目为基于 Debian 和 Red Hat 的 Linux 发行版以及一些不提供包管理器的发行版提供通用的指令
* 每台机器 2 GB 或更多的 RAM （如果少于这个数字将会影响你应用的运行内存)
2 CPU 核或更多
* 集群中的所有机器的网络彼此均能相互连接(公网和内网都可以)
* 节点之中不可以有重复的主机名、MAC 地址或 product_uuid。请参见这里了解更多详细信息。
*开启机器上的某些端口。请参见这里 了解更多详细信息。
* 禁用交换分区。为了保证 kubelet 正常工作，你 必须 禁用交换分区。

		Kubernetes 1.8开始要求关闭系统的Swap，如果不关闭，默认配置下kubelet将无法启动，关闭系统的Swap方法如下:
	
		swapoff -a
		修改 /etc/fstab 文件，注释掉 SWAP 的自动挂载，使用free -m确认swap已经关闭。 swappiness参数调整，修改/etc/sysctl.d/k8s.conf添加下面一行：
		
		vm.swappiness=0
		执行sysctl -p /etc/sysctl.d/k8s.conf使修改生效。

 ![所需端口](k8s/0.jpg)

***
### 前置工作

#### 确保每个节点上 MAC 地址和 product_uuid 的唯一性

* 你可以使用命令 ip link 或 ifconfig -a 来获取网络接口的 MAC 地址
* 可以使用 sudo cat /sys/class/dmi/id/product_uuid 命令对 product_uuid 校验


一般来讲，硬件设备会拥有唯一的地址，但是有些虚拟机的地址可能会重复。 Kubernetes 使用这些值来唯一确定集群中的节点。 如果这些值在每个节点上不唯一，可能会导致安装 失败。

#### 检查网络适配器 
如果你有一个以上的网络适配器，同时你的 Kubernetes 组件通过默认路由不可达，我们建议你预先添加 IP 路由规则，这样 Kubernetes 集群就可以通过对应的适配器完成连接。

#### 允许 iptables 检查桥接流量
确保 br_netfilter 模块被加载。这一操作可以通过运行 lsmod | grep br_netfilter 来完成。若要显式加载该模块，可执行 sudo modprobe br_netfilter。

为了让你的 Linux 节点上的 iptables 能够正确地查看桥接流量，你需要确保在你的 sysctl 配置中将 net.bridge.bridge-nf-call-iptables 设置为 1。例如：

	cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
	br_netfilter
	EOF
	
	cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
	net.bridge.bridge-nf-call-ip6tables = 1
	net.bridge.bridge-nf-call-iptables = 1
	EOF
	sudo sysctl --system

#### 安装 runtime
略,见安装docker
修改systemd
vi /etc/docker/daemon.json 
{
  "default-runtime":"nvidia",
  "runtimes": {
    "nvidia": {
      "path": "nvidia-container-runtime",
      "runtimeArgs": []
    }
  },
  "exec-opts": ["native.cgroupdriver=systemd"],
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "100m"
  },
  "storage-driver": "overlay2",
  "storage-opts": [
    "overlay2.override_kernel_check=true"
  ]
}

systemctl restart docker

可忽略:搭建本地仓库:

* docker pull registry
* docker run -d -v C:/develop/dockerimages:/var/lib/registry -p 5000:5000 --restart=always --name pewee-registry registry
* 修改要传镜像的docker: vi /etc/docker/daemon.json

	  	{ 
	    	"insecure-registries" : [ "your-server-ip:5000" ], 
		}

* systemctl restart docker
* docker tag your-image-name:version your-server-ip:5000/your-image-name:version
* docker push your-registry-server-ip:5000/your-image-name:version
* 
***
### 在线部署kubeadm

#### 安装 kubeadm、kubelet 和 kubectl 
在每台机器上安装以下的软件包：

* kubeadm：用来初始化集群的指令。

* kubelet：在集群中的每个节点上用来启动 Pod 和容器等。

* kubectl：用来与集群通信的命令行工具。

		cat <<EOF | sudo tee /etc/yum.repos.d/kubernetes.repo
		[kubernetes]
		name=Kubernetes
		baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-\$basearch
		enabled=1
		gpgcheck=1
		repo_gpgcheck=1
		gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
		exclude=kubelet kubeadm kubectl
		EOF
		
		# 将 SELinux 设置为 permissive 模式（相当于将其禁用）
		sudo setenforce 0
		sudo sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config
		
		sudo yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes
		
		sudo systemctl enable --now kubelet

注意!!国内源替换为https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/

centos7用户还需要设置路由：

cat <<EOF >  /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF
sysctl --system

#### 配置节点
控制平面节点是运行控制平面组件的机器， 包括 etcd （集群数据库） 和 API Server （命令行工具 kubectl 与之通信）。

	1. kubeadm init 启动一个 Kubernetes 主节点
	2. kubeadm join 启动一个 Kubernetes 工作节点并且将其加入到集群
	3. kubeadm upgrade 更新一个 Kubernetes 集群到新版本
	4. kubeadm config 如果使用 v1.7.x 或者更低版本的 kubeadm 初始化集群，您需要对集群做一些配置以便使用 kubeadm upgrade 命令
	5. kubeadm token 管理 kubeadm join 使用的令牌
	6. kubeadm reset 还原 kubeadm init 或者 kubeadm join 对主机所做的任何更改

添加文件kubeadm.yaml
apiVersion: kubeadm.k8s.io/v1beta2
kind: ClusterConfiguration
controllerManager:
  extraArgs:
    horizontal-pod-autoscaler-use-rest-clients: "true"
    horizontal-pod-autoscaler-sync-period: "10s"
    node-monitor-grace-period: "10s"
apiServer:
  extraArgs:
    runtime-config: "api/all=true"
kubernetesVersion: v1.22.2
cgroupDriver: systemd

#### 拉取镜像
首先使用下面的命令获取需要的docker镜像名称：
	
	kubeadm config images list
	[root@localhost ~]# kubeadm config images list
	k8s.gcr.io/kube-apiserver:v1.22.2
	k8s.gcr.io/kube-controller-manager:v1.22.2
	k8s.gcr.io/kube-scheduler:v1.22.2
	k8s.gcr.io/kube-proxy:v1.22.2
	k8s.gcr.io/pause:3.5
	k8s.gcr.io/etcd:3.5.0-0
	k8s.gcr.io/coredns/coredns:v1.8.4

编写脚本：vi pull_k8s_images.sh

	set -o errexit
	set -o nounset
	set -o pipefail
	
	##这里定义版本，按照上面得到的列表自己改一下版本号
	
	KUBE_VERSION=v1.22.2
	KUBE_PAUSE_VERSION=3.5
	ETCD_VERSION=3.5.0-0
	DNS_VERSION=v1.8.4
	
	##这是原始仓库名，最后需要改名成这个
	GCR_URL=k8s.gcr.io
	
	##这里就是写你要使用的仓库
	DOCKERHUB_URL=gotok8s
	
	##这里是镜像列表，新版本要把coredns改成coredns/coredns
	images=(
	kube-proxy:${KUBE_VERSION}
	kube-scheduler:${KUBE_VERSION}
	kube-controller-manager:${KUBE_VERSION}
	kube-apiserver:${KUBE_VERSION}
	pause:${KUBE_PAUSE_VERSION}
	etcd:${ETCD_VERSION}
	coredns:${DNS_VERSION}
	)
	
	##这里是拉取和改名的循环语句
	for imageName in ${images[@]} ; do
	  docker pull $DOCKERHUB_URL/$imageName
	  docker tag $DOCKERHUB_URL/$imageName $GCR_URL/$imageName
	  docker rmi $DOCKERHUB_URL/$imageName
	done

执行 :chmod +x ./pull_k8s_images.sh && ./pull_k8s_images.sh

执行:kubeadm init  --config kubeadm.yaml
发现报错:failed to pull image k8s.gcr.io/coredns/coredns:v1.8.4

我们下载的镜像和需要的镜像名字不同,修改一下:
	
	docker tag k8s.gcr.io/coredns:v1.8.4 k8s.gcr.io/coredns/coredns:v1.8.4
在执行: kubeadm init  --config kubeadm.yaml

!!未成功

### 离线部署

在https://github.com/lework/kainstall-offline/releases下载安装包打包
下载脚本:https://github.com/lework/kainstall
wget https://cdn.jsdelivr.net/gh/lework/kainstall@master/kainstall-centos.sh

安装集群:
bash kainstall-centos.sh init \
  --master 192.168.111.3,192.168.77.131,192.168.77.132 \
  --worker 192.168.77.133,192.168.77.134 \
  --user root \
  --password 123456 \
  --offline-file 1.20.6_centos7.tgz

## 常用命令
* $ kubectl create -f 我的配置文件

		apiVersion: apps/v1
		kind: Deployment
		metadata:
		  name: nginx-deployment
		spec:
		  selector:
		    matchLabels:
		      app: nginx
		  replicas: 2
		  template:
		    metadata:
		      labels:
		        app: nginx
		    spec:
		      containers:
		      - name: nginx
		        image: nginx:1.7.9
		        ports:
		        - containerPort: 80

    * Kind 字段，指定了这个 API 对象的类型（Type），是一个 Deployment。是一个定义多副本应用（即多个副本 Pod）的对象,此外，Deployment 还负责在 Pod 定义发生变化时，对每个副本进行滚动更新（Rolling Update）。在上面这个 YAML 文件中，我给它定义的 Pod 副本个数 (spec.replicas) 是：2。
    * Pod 模版（spec.template）

* $ kubectl describe pod ingress-demo-app-694bf5d965-2d5w5  资源 .如node ,pod ,最下方查看到事件schedule,pull,create,start
* $ kubectl get pods -l app=nginx 获取标签为app=nginx
* $ kubectl apply -f nginx-deployment.yaml 

    当对文件更新(版本,副本数)

### 实战
#### 部署我的微服务镜像
 
    > docker pull 192.168.111.1:5000/bsp:1.0 


apiVersion: apps/v1
kind: Deployment
metadata:
  name: bsp-deployment
spec:
  selector:
    matchLabels:
      app: bsp
  replicas: 1
  template:
    metadata:
      labels:
        app: bsp
    spec:
      containers:
      - name: bsp
        image: 192.168.111.1:5000/bsp:1.0
        env:
	    - name: SERVER_PORT
	      value: "80"
        ports:
        - containerPort: 80
        volumeMounts:
        - mountPath: "/root/logs/bsp/"
          name: bsp-log
      volumes:
      - name: bsp-log
        hostPath: 
          path: "C:/Users/pewee/logs/bsp/"

含义解释:
将宿主机 C:/Users/pewee/logs/bsp/ 挂载到容器/root/logs/bsp/
在pod中定义的volumes,其中的所有container都可以使用

#### initcontainer

	apiVersion: v1
	kind: Pod
	metadata:
	  name: javaweb-2
	spec:
	  initContainers:
	  - image: geektime/sample:v2
	    name: war
	    command: ["cp", "/sample.war", "/app"]
	    volumeMounts:
	    - mountPath: /app
	      name: app-volume
	  containers:
	  - image: geektime/tomcat:7.0
	    name: tomcat
	    command: ["sh","-c","/root/apache-tomcat-7.0.42-v2/bin/start.sh"]
	    volumeMounts:
	    - mountPath: /root/apache-tomcat-7.0.42-v2/webapps
	      name: app-volume
	    ports:
	    - containerPort: 8080
	      hostPort: 8001 
	  volumes:
	  - name: app-volume
	    emptyDir: {}

* 在 Pod 中，所有 Init Container 定义的容器，都会比 spec.containers 定义的用户容器先启动。并且，Init Container 容器会按顺序逐一启动，而直到它们都启动并且退出了，用户容器才会启动。所以，这个 Init Container 类型的 WAR 包容器启动后，我执行了一句"cp /sample.war /app"，把应用的 WAR 包拷贝到 /app 目录下，然后退出。而后这个 /app 目录，就挂载了一个名叫 app-volume 的 Volume。接下来就很关键了。
* Tomcat 容器，同样声明了挂载 app-volume 到自己的 webapps 目录下。所以，等 Tomcat 容器启动时，它的 webapps 目录下就一定会存在 sample.war 文件：这个文件正是 WAR 包容器启动时拷贝到这个 Volume 里面的，而这个 Volume 是被这两个容器共享的。

#### 容器的日志收集

比如，我现在有一个应用，需要不断地把日志文件输出到容器的 /var/log 目录中。这时，我就可以把一个 Pod 里的 Volume 挂载到应用容器的 /var/log 目录上。然后，我在这个 Pod 里同时运行一个 sidecar 容器，它也声明挂载同一个 Volume 到自己的 /var/log 目录上。这样，接下来 sidecar 容器就只需要做一件事儿，那就是不断地从自己的 /var/log 目录里读取日志文件，转发到 MongoDB 或者 Elasticsearch 中存储起来。这样，一个最基本的日志收集工作就完成了。\

### pod

* NodeSelector：是一个供用户将 Pod 与 Node 进行绑定的字段

	apiVersion: v1
	kind: Pod
	...
	spec:
	 nodeSelector:
	   disktype: ssd

这样的一个配置，意味着这个 Pod 永远只能运行在携带了“disktype: ssd”标签（Label）的节点上；否则，它将调度失败。

* HostAliases：定义了 Pod 的 hosts 文件（比如 /etc/hosts）里的内容


		apiVersion: v1
		kind: Pod
		...
		spec:
		  hostAliases:
		  - ip: "10.1.2.3"
		    hostnames:
		    - "foo.remote"
		    - "bar.remote"
		...
在这个 Pod 的 YAML 文件中，我设置了一组 IP 和 hostname 的数据。这样，这个 Pod 启动后，/etc/hosts 文件的内容将如下所示：

		cat /etc/hosts
		# Kubernetes-managed hosts file.
		127.0.0.1 localhost
		...
		10.244.135.10 hostaliases-pod
		10.1.2.3 foo.remote
		10.1.2.3 bar.remote
在 Kubernetes 项目中，如果要设置 hosts 文件里的内容，一定要通过这种方法。否则，如果直接修改了 hosts 文件的话，在 Pod 被删除重建之后，kubelet 会自动覆盖掉被修改的内容。

* Pod 对象在 Kubernetes 中的生命周期。

	  * Pending。这个状态意味着，Pod 的 YAML 文件已经提交给了 Kubernetes，API 对象已经被创建并保存在 Etcd 当中。但是，这个 Pod 里有些容器因为某种原因而不能被顺利创建。比如，调度不成功。
	  * Running。这个状态下，Pod 已经调度成功，跟一个具体的节点绑定。它包含的容器都已经创建成功，并且至少有一个正在运行中。
	  * Succeeded。这个状态意味着，Pod 里的所有容器都正常运行完毕，并且已经退出了。这种情况在运行一次性任务时最为常见。
	  * Failed。这个状态下，Pod 里至少有一个容器以不正常的状态（非 0 的返回码）退出。这个状态的出现，意味着你得想办法 Debug 这个容器的应用，比如查看 Pod 的 Events 和日志。
	  * Unknown。这是一个异常状态，意味着 Pod 的状态不能持续地被 kubelet 汇报给 kube-apiserver，这很有可能是主从节点（Master 和 Kubelet）间的通信出现了问题。

### Volume
1. Secret:
Secret 最典型的使用场景，莫过于存放数据库的 Credential 信息

		apiVersion: v1
		kind: Pod
		metadata:
		  name: test-projected-volume 
		spec:
		  containers:
		  - name: test-secret-volume
		    image: busybox
		    args:
		    - sleep
		    - "86400"
		    volumeMounts:
		    - name: mysql-cred
		      mountPath: "/projected-volume"
		      readOnly: true
		  volumes:
		  - name: mysql-cred
		    projected:
		      sources:
		      - secret:
		          name: user
		      - secret:
		          name: pass
这里我定义了一个简单的容器。它声明挂载的 Volume，并不是常见的 emptyDir 或者 hostPath 类型，而是 projected 类型。而这个 Volume 的数据来源（sources），则是名为 user 和 pass 的 Secret 对象，分别对应的是数据库的用户名和密码。
设置密码

		$ echo -n 'admin' | base64
		YWRtaW4=
		$ echo -n '1f2d1e2e67df' | base64
		MWYyZDFlMmU2N2Rm
通过编写 YAML 文件的方式来创建这个 Secret 对象

		apiVersion: v1
		kind: Secret
		metadata:
		  name: mysecret
		type: Opaque
		data:
		  user: YWRtaW4=
		  pass: MWYyZDFlMmU2N2Rm
创建这个 Pod：kubectl create -f test-projected-volume.yaml

		$ kubectl exec -it test-projected-volume -- /bin/sh
		$ ls /projected-volume/
		user
		pass
		$ cat /projected-volume/user
		root
		$ cat /projected-volume/pass
		1f2d1e2e67df

2. ConfigMap；
 * 与 Secret 的区别在于，ConfigMap 保存的是不需要加密的、应用所需的配置信息。而 ConfigMap 的用法几乎与 Secret 完全相同：你可以使用 kubectl create configmap 从文件或者目录创建 ConfigMap，也可以直接编写 ConfigMap 对象的 YAML 文件。
 * 一个 Java 应用所需的配置文件（.properties 文件），就可以通过下面这样的方式保存在 ConfigMap 里：
	
			# .properties文件的内容
			$ cat example/ui.properties
			color.good=purple
			color.bad=yellow
			allow.textmode=true
			how.nice.to.look=fairlyNice
				
			# 从.properties文件创建ConfigMap
			$ kubectl create configmap ui-config --from-file=example/ui.properties
				
			# 查看这个ConfigMap里保存的信息(data)
			$ kubectl get configmaps ui-config -o yaml
			apiVersion: v1
			data:
			  ui.properties: |
			    color.good=purple
				    color.bad=yellow
			    allow.textmode=true
			    how.nice.to.look=fairlyNice
			kind: ConfigMap
			metadata:
			  name: ui-config
			  ...

3. Downward API:Pod 里的容器能够直接获取到这个 Pod API 对象本身的信息。
例子:

		apiVersion: v1
		kind: Pod
		metadata:
		  name: test-downwardapi-volume
		  labels:
		    zone: us-est-coast
		    cluster: test-cluster1
		    rack: rack-22
		spec:
		  containers:
		    - name: client-container
		      image: k8s.gcr.io/busybox
		      command: ["sh", "-c"]
		      args:
		      - while true; do
		          if [[ -e /etc/podinfo/labels ]]; then
		            echo -en '\n\n'; cat /etc/podinfo/labels; fi;
		          sleep 5;
		        done;
		      volumeMounts:
		        - name: podinfo
		          mountPath: /etc/podinfo
		          readOnly: false
		  volumes:
		    - name: podinfo
		      projected:
		        sources:
		        - downwardAPI:
		            items:
		              - path: "labels"
		                fieldRef:
		                  fieldPath: metadata.labels
这里把pod的metadata.labels的内容作为labels文件在container中挂载到了目录下读取:

		$ kubectl create -f dapi-volume.yaml
		$ kubectl logs test-downwardapi-volume
		cluster="test-cluster1"
		rack="rack-22"
		zone="us-est-coast"

		1.使用fieldRef可以声明使用:
		spec.nodeName - 宿主机名字
		status.hostIP - 宿主机IP
		metadata.name - Pod的名字
		metadata.namespace - Pod的Namespace
		status.podIP - Pod的IP
		spec.serviceAccountName - Pod的Service Account的名字
		metadata.uid - Pod的UID
		metadata.labels['<KEY>'] - 指定<KEY>的Label值
		metadata.annotations['<KEY>'] - 指定<KEY>的Annotation值
		metadata.labels - Pod的所有Label
		metadata.annotations - Pod的所有Annotation
		
		2. 使用resourceFieldRef可以声明使用:
		容器的CPU limit
		容器的CPU request
		容器的memory limit
		容器的memory request

4. ServiceAccountToken :
    >任何运行在 Kubernetes 集群上的应用，都必须使用这个 ServiceAccountToken 里保存的授权信息，也就是 Token，才可以合法地访问 API Server,为了方便使用，Kubernetes 已经为你提供了一个默认“服务账户”（default Service Account）。并且，任何一个运行在 Kubernetes 里的 Pod，都可以直接使用这个默认的 Service Account，而无需显示地声明挂载它。

		$ kubectl describe pod nginx-deployment-5c678cfb6d-lg9lw
		Containers:
		...
		  Mounts:
		    /var/run/secrets/kubernetes.io/serviceaccount from default-token-s8rbq (ro)
		Volumes:
		  default-token-s8rbq:
		  Type:       Secret (a volume populated by a Secret)
		  SecretName:  default-token-s8rbq
		  Optional:    false
   一旦 Pod 创建完成，容器里的应用就可以直接从这个默认 ServiceAccountToken 的挂载目录里访问到授权信息和文件。这个容器内的路径在 Kubernetes 里是固定的，即：/var/run/secrets/kubernetes.io/serviceaccount

### 探针Probe
>在 Kubernetes 中，你可以为 Pod 里的容器定义一个健康检查“探针”（Probe）。这样，kubelet 就会根据这个 Probe 的返回值决定这个容器的状态，而不是直接以容器镜像是否运行（来自 Docker 返回的信息）作为依据。这种机制，是生产环境中保证应用健康存活的重要手段。
	
	apiVersion: v1
	kind: Pod
	metadata:
	  labels:
	    test: liveness
	  name: test-liveness-exec
	spec:
	  containers:
	  - name: liveness
	    image: busybox
	    args:
	    - /bin/sh
	    - -c
	    - touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600
	    livenessProbe:
	      exec:
	        command:
	        - cat
	        - /tmp/healthy
	      initialDelaySeconds: 5
	      periodSeconds: 5
在这个 Pod 中，我们定义了一个有趣的容器。它在启动之后做的第一件事，就是在 /tmp 目录下创建了一个 healthy 文件，以此作为自己已经正常运行的标志。而 30 s 过后，它会把这个文件删除掉。与此同时，我们定义了一个这样的 livenessProbe（健康检查）。它的类型是 exec，这意味着，它会在容器启动后，在容器里面执行一条我们指定的命令，比如：“cat /tmp/healthy”。这时，如果这个文件存在，这条命令的返回值就是 0，Pod 就会认为这个容器不仅已经启动，而且是健康的。这个健康检查，在容器启动 5 s 后开始执行（initialDelaySeconds: 5），每 5 s 执行一次（periodSeconds: 5）。

kubectl create -f test-liveness-exec.yaml
kubectl get pod
kubectl describe pod test-liveness-exec
发现，这个 Pod 在 Events 报告了一个异常：

显然，这个健康检查探查到 /tmp/healthy 已经不存在了，所以它报告容器是不健康的。那么接下来会发生什么呢？

我们不妨再次查看一下这个 Pod 的状态：发现，Pod 并没有进入 Failed 状态，而是保持了 Running 状态
RESTARTS 字段从 0 到 1 的变化，就明白原因了：这个异常的容器已经被 Kubernetes 重启了

Pod 的恢复过程，永远都是发生在当前节点上，而不会跑到别的节点上去。事实上，一旦一个 Pod 与一个节点（Node）绑定，除非这个绑定发生了变化（pod.spec.node 字段被修改），否则它永远都不会离开这个节点。这也就意味着，如果这个宿主机宕机了，这个 Pod 也不会主动迁移到其他节点上去。而如果你

想让 Pod 出现在其他的可用节点上，就必须使用 Deployment 这样的“控制器”来管理 Pod，哪怕你只需要一个 Pod 副本。	

	...
	livenessProbe:
	     httpGet:
	       path: /healthz
	       port: 8080
	       httpHeaders:
	       - name: X-Custom-Header
	         value: Awesome
	       initialDelaySeconds: 3
	       periodSeconds: 3

### PodPreset


开发人员编写简单的编排

	apiVersion: v1
	kind: Pod
	metadata:
	  name: website
	  labels:
	    app: website
	    role: frontend
	spec:
	  containers:
	    - name: website
	      image: nginx
	      ports:
	        - containerPort: 80
运维人员定义好模板:preset.yaml

	apiVersion: settings.k8s.io/v1alpha1
	kind: PodPreset
	metadata:
	  name: allow-database
	spec:
	  selector:
	    matchLabels:
	      role: frontend
	  env:
	    - name: DB_PORT
	      value: "6379"
	  volumeMounts:
	    - mountPath: /cache
	      name: cache-volume
	  volumes:
	    - name: cache-volume
	      emptyDir: {}
运维人员先创建了这个 PodPreset，然后开发人员才创建 Pod：

	$ kubectl get pod website -o yaml
	apiVersion: v1
	kind: Pod
	metadata:
	  name: website
	  labels:
	    app: website
	    role: frontend
	  annotations:
	    podpreset.admission.kubernetes.io/podpreset-allow-database: "resource version"
	spec:
	  containers:
	    - name: website
	      image: nginx
	      volumeMounts:
	        - mountPath: /cache
	          name: cache-volume
	      ports:
	        - containerPort: 80
	      env:
	        - name: DB_PORT
	          value: "6379"
	  volumes:
	    - name: cache-volume
	      emptyDir: {}
