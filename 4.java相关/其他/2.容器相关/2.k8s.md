# k8s
***

## 基本概念
1. master 节点:负责整个集群控制,管理,有以下关键组件

  * kube-apiserver:k8s所有资源crud的接口,是控制集群的入口
  * kube-controller-manager:k8s里所有资源的自动化控制中心
  * kube-seheduler:负责pod调度的进程
2. node 计算节点,负责工作负载,有以下关键组件

  * kubelet:负责pod创建,启停.与master协作管理集群
  * kube-proxy:实现k8s service的通信和lb
  * docker engine:负责本机容器创建和管理


## 部署篇:kubeadm

准备开始

* 一台兼容的 Linux 主机。Kubernetes 项目为基于 Debian 和 Red Hat 的 Linux 发行版以及一些不提供包管理器的发行版提供通用的指令
* 每台机器 2 GB 或更多的 RAM （如果少于这个数字将会影响你应用的运行内存)
2 CPU 核或更多
* 集群中的所有机器的网络彼此均能相互连接(公网和内网都可以)
* 节点之中不可以有重复的主机名、MAC 地址或 product_uuid。请参见这里了解更多详细信息。
*开启机器上的某些端口。请参见这里 了解更多详细信息。
* 禁用交换分区。为了保证 kubelet 正常工作，你 必须 禁用交换分区。

		Kubernetes 1.8开始要求关闭系统的Swap，如果不关闭，默认配置下kubelet将无法启动，关闭系统的Swap方法如下:
	
		swapoff -a
		修改 /etc/fstab 文件，注释掉 SWAP 的自动挂载，使用free -m确认swap已经关闭。 swappiness参数调整，修改/etc/sysctl.d/k8s.conf添加下面一行：
		
		vm.swappiness=0
		执行sysctl -p /etc/sysctl.d/k8s.conf使修改生效。

 ![所需端口](k8s/0.jpg)

***
### 前置工作

#### 确保每个节点上 MAC 地址和 product_uuid 的唯一性

* 你可以使用命令 ip link 或 ifconfig -a 来获取网络接口的 MAC 地址
* 可以使用 sudo cat /sys/class/dmi/id/product_uuid 命令对 product_uuid 校验


一般来讲，硬件设备会拥有唯一的地址，但是有些虚拟机的地址可能会重复。 Kubernetes 使用这些值来唯一确定集群中的节点。 如果这些值在每个节点上不唯一，可能会导致安装 失败。

#### 检查网络适配器 
如果你有一个以上的网络适配器，同时你的 Kubernetes 组件通过默认路由不可达，我们建议你预先添加 IP 路由规则，这样 Kubernetes 集群就可以通过对应的适配器完成连接。

#### 允许 iptables 检查桥接流量
确保 br_netfilter 模块被加载。这一操作可以通过运行 lsmod | grep br_netfilter 来完成。若要显式加载该模块，可执行 sudo modprobe br_netfilter。

为了让你的 Linux 节点上的 iptables 能够正确地查看桥接流量，你需要确保在你的 sysctl 配置中将 net.bridge.bridge-nf-call-iptables 设置为 1。例如：

	cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
	br_netfilter
	EOF
	
	cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
	net.bridge.bridge-nf-call-ip6tables = 1
	net.bridge.bridge-nf-call-iptables = 1
	EOF
	sudo sysctl --system

centos7用户还需要设置路由：

cat <<EOF >  /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF
sysctl --system

#### 安装 runtime
略,见安装docker
修改systemd
vi /etc/docker/daemon.json 
{
  "exec-opts": ["native.cgroupdriver=systemd"],
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "100m"
  },
  "storage-driver": "overlay2",
  "storage-opts": [
    "overlay2.override_kernel_check=true"
  ],
  "registry-mirrors": ["https://lcs5rvt6.mirror.aliyuncs.com"]
}

可选源修改:
# 创建或修改 /etc/docker/daemon.json 文件，修改为如下形式
{
    "registry-mirrors" : [
    "https://registry.docker-cn.com",
    "https://docker.mirrors.ustc.edu.cn",
    "http://hub-mirror.c.163.com",
    "https://cr.console.aliyun.com/"
  ]
}
# 重启docker服务使配置生效
$ systemctl restart docker.service
版权声明：本文为CSDN博主「zlzhaoe」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/u012081441/article/details/104553145/

systemctl restart docker

可忽略:搭建本地仓库:

* docker pull registry
* docker run -d -v C:/develop/dockerimages:/var/lib/registry -p 5000:5000 --restart=always --name pewee-registry registry
* 修改要传镜像的docker: vi /etc/docker/daemon.json

	  	{ 
	    	"insecure-registries" : [ "your-server-ip:5000" ], 
		}

* systemctl restart docker
* docker tag your-image-name:version your-server-ip:5000/your-image-name:version
* docker push your-registry-server-ip:5000/your-image-name:version
* 
***
### 在线部署kubeadm

#### 准备

修改host

	hostnamectl set-hostname <hostname>

关闭swap

	swapoff -a  # 临时
	sed -ri 's/.*swap.*/#&/' /etc/fstab    # 永久

在master添加hosts

cat >> /etc/hosts << EOF
192.168.44.146 k8smaster
192.168.44.145 k8snode1
192.168.44.144 k8snode2
EOF

#### 安装 kubeadm、kubelet 和 kubectl 
在每台机器上安装以下的软件包：

* kubeadm：用来初始化集群的指令。

* kubelet：在集群中的每个节点上用来启动 Pod 和容器等。

* kubectl：用来与集群通信的命令行工具。
		
		#设置k8s仓库源
		cat <<EOF | sudo tee /etc/yum.repos.d/kubernetes.repo
		[kubernetes]
		name=Kubernetes
		baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-\$basearch
		enabled=1
		gpgcheck=1
		repo_gpgcheck=1
		gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
		exclude=kubelet kubeadm kubectl
		EOF
		注意!!国内源替换
		cat <<EOF > /etc/yum.repos.d/kubernetes.repo
		[kubernetes]
		name=Kubernetes
		baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
		enabled=1
		gpgcheck=1
		repo_gpgcheck=1
		gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
		EOF
		
		# 将 SELinux 设置为 permissive 模式（相当于将其禁用）
		sudo setenforce 0
		sudo sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config
		# 安装三套件
		sudo yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes
		
		sudo systemctl enable --now kubelet


#### 配置节点
控制平面节点是运行控制平面组件的机器， 包括 etcd （集群数据库） 和 API Server （命令行工具 kubectl 与之通信）。

	1. kubeadm init 启动一个 Kubernetes 主节点
	2. kubeadm join 启动一个 Kubernetes 工作节点并且将其加入到集群
	3. kubeadm upgrade 更新一个 Kubernetes 集群到新版本
	4. kubeadm config 如果使用 v1.7.x 或者更低版本的 kubeadm 初始化集群，您需要对集群做一些配置以便使用 kubeadm upgrade 命令
	5. kubeadm token 管理 kubeadm join 使用的令牌
	6. kubeadm reset 还原 kubeadm init 或者 kubeadm join 对主机所做的任何更改

添加文件kubeadm.yaml
apiVersion: kubeadm.k8s.io/v1beta2
kind: ClusterConfiguration
controllerManager:
  extraArgs:
    horizontal-pod-autoscaler-use-rest-clients: "true"
    horizontal-pod-autoscaler-sync-period: "10s"
    node-monitor-grace-period: "10s"
apiServer:
  extraArgs:
    runtime-config: "api/all=true"
kubernetesVersion: v1.22.2
cgroupDriver: systemd

当然也可以直接以命令行的形式

192.168.1.201是本机ip，其他地址是k8规划地址。k8版本，按照安装的版本来填
kubeadm init \
--apiserver-advertise-address=192.168.1.111 \
--image-repository registry.aliyuncs.com/google_containers \
--kubernetes-version v1.22.2 \
--service-cidr=10.1.0.0/16 \
--pod-network-cidr=10.50.0.0/16

结果成功


	Your Kubernetes control-plane has initialized successfully!
	
	To start using your cluster, you need to run the following as a regular user:
	
	  mkdir -p $HOME/.kube
	  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
	  sudo chown $(id -u):$(id -g) $HOME/.kube/config
	
	Alternatively, if you are the root user, you can run:
	
	  export KUBECONFIG=/etc/kubernetes/admin.conf
	
	You should now deploy a pod network to the cluster.
	Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
	  https://kubernetes.io/docs/concepts/cluster-administration/addons/
	
	Then you can join any number of worker nodes by running the following on each as root:
	
	kubeadm join 192.168.1.111:6443 --token usjurd.zkwpc8hhy4nshde7 \
	        --discovery-token-ca-cert-hash sha256:8a6c3d59ef8fd6db2114b007a3e3761d2e0dfc7de1516acbd92b800a142372b2

再导出配置

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

# 镜像拉不下来，错误信息如下，因为阿里云这没有此镜像，需要手动下载镜像，然后打tag
# [ERROR ImagePull]: failed to pull image registry.aliyuncs.com/google_containers/coredns:v1.8.4: output: Error response from daemon: manifest for registry.aliyuncs.com/google_containers/coredns:v1.8.4 not found: manifest unknown: manifest unknown
https://blog.51cto.com/8999a/2784605

docker pull coredns/coredns
docker tag coredns/coredns:latest registry.aliyuncs.com/google_containers/coredns:v1.8.4

# 查看初始化集群时，需要拉的镜像名
kubeadm config images list

#### 拉取镜像
首先使用下面的命令获取需要的docker镜像名称：
	
	kubeadm config images list
	[root@localhost ~]# kubeadm config images list
	k8s.gcr.io/kube-apiserver:v1.22.2
	k8s.gcr.io/kube-controller-manager:v1.22.2
	k8s.gcr.io/kube-scheduler:v1.22.2
	k8s.gcr.io/kube-proxy:v1.22.2
	k8s.gcr.io/pause:3.5
	k8s.gcr.io/etcd:3.5.0-0
	k8s.gcr.io/coredns/coredns:v1.8.4

编写脚本：vi pull_k8s_images.sh

	set -o errexit
	set -o nounset
	set -o pipefail
	
	##这里定义版本，按照上面得到的列表自己改一下版本号
	
	KUBE_VERSION=v1.22.2
	KUBE_PAUSE_VERSION=3.5
	ETCD_VERSION=3.5.0-0
	DNS_VERSION=v1.8.4
	
	##这是原始仓库名，最后需要改名成这个
	GCR_URL=k8s.gcr.io
	
	##这里就是写你要使用的仓库
	DOCKERHUB_URL=gotok8s
	
	##这里是镜像列表，新版本要把coredns改成coredns/coredns
	images=(
	kube-proxy:${KUBE_VERSION}
	kube-scheduler:${KUBE_VERSION}
	kube-controller-manager:${KUBE_VERSION}
	kube-apiserver:${KUBE_VERSION}
	pause:${KUBE_PAUSE_VERSION}
	etcd:${ETCD_VERSION}
	coredns:${DNS_VERSION}
	)
	
	##这里是拉取和改名的循环语句
	for imageName in ${images[@]} ; do
	  docker pull $DOCKERHUB_URL/$imageName
	  docker tag $DOCKERHUB_URL/$imageName $GCR_URL/$imageName
	  docker rmi $DOCKERHUB_URL/$imageName
	done

执行 :chmod +x ./pull_k8s_images.sh && ./pull_k8s_images.sh

执行:kubeadm init  --config kubeadm.yaml
发现报错:failed to pull image k8s.gcr.io/coredns/coredns:v1.8.4

我们下载的镜像和需要的镜像名字不同,修改一下:
	
	docker tag k8s.gcr.io/coredns:v1.8.4 k8s.gcr.io/coredns/coredns:v1.8.4
在执行: kubeadm init  --config kubeadm.yaml

#### 安装网络flannel
wget https://raw.githubusercontent.com/coreos/flannel/v0.15.1/Documentation/kube-flannel.yml

阿里云和腾讯云可以配置VPC,参考https://github.com/flannel-io/flannel/blob/master/Documentation/alicloud-vpc-backend-cn.md
配置key和secret,配置etcd
修改在腾讯云backend-type is "tencent-vpc"
否则使用默认配置或者host-gw

kubectl apply -f flannel.yml

### 离线部署

在https://github.com/lework/kainstall-offline/releases下载安装包打包
下载脚本:https://github.com/lework/kainstall
wget https://cdn.jsdelivr.net/gh/lework/kainstall@master/kainstall-centos.sh

安装集群:
bash kainstall-centos.sh init \
  --master 192.168.111.3,192.168.77.131,192.168.77.132 \
  --worker 192.168.77.133,192.168.77.134 \
  --user root \
  --password 123456 \
  --offline-file 1.20.6_centos7.tgz

## 常用命令
* $ kubectl create -f 我的配置文件

		apiVersion: apps/v1
		kind: Deployment
		metadata:
		  name: nginx-deployment
		spec:
		  selector:
		    matchLabels:
		      app: nginx
		  replicas: 2
		  template:
		    metadata:
		      labels:
		        app: nginx
		    spec:
		      containers:
		      - name: nginx
		        image: nginx:1.7.9
		        ports:
		        - containerPort: 80

    * Kind 字段，指定了这个 API 对象的类型（Type），是一个 Deployment。是一个定义多副本应用（即多个副本 Pod）的对象,此外，Deployment 还负责在 Pod 定义发生变化时，对每个副本进行滚动更新（Rolling Update）。在上面这个 YAML 文件中，我给它定义的 Pod 副本个数 (spec.replicas) 是：2。
    * Pod 模版（spec.template）

* $ kubectl describe pod ingress-demo-app-694bf5d965-2d5w5  资源 .如node ,pod ,最下方查看到事件schedule,pull,create,start
* $ kubectl get pods -l app=nginx 获取标签为app=nginx
* $ kubectl apply -f nginx-deployment.yaml 

    当对文件更新(版本,副本数)

	kubectl apply service kubernetes-dashboard -p '{"spec":{"type":"NodePort"}}' -n kube-dashboard  #以打补丁方式修改dasboard的访问方式
	service/kubernetes-dashboard patched

#### 常见问题
	[root@VM_0_3_centos ~]# kubectl get pod
	The connection to the server localhost:8080 was refused - did you specify the right host or port?
解决方法:

* master:

		方式一：编辑文件设置
		   vim /etc/profile
		   在底部增加新的环境变量 export KUBECONFIG=/etc/kubernetes/admin.conf
		方式二:直接追加文件内容
		echo "export KUBECONFIG=/etc/kubernetes/admin.conf" >> /etc/profile
	
		source /etc/profile
* node :

		将主节点（master节点）中的【/etc/kubernetes/admin.conf】文件拷贝到从节点相同目录下:
		scp -r /etc/kubernetes/admin.conf ${node1}:/etc/kubernetes/admin.conf
		echo “export KUBECONFIG=/etc/kubernetes/admin.conf” >> ~/.bash_profile
		source ~/.bash_profile

### 实战
#### 部署我的微服务镜像
 
    > docker pull 192.168.111.1:5000/bsp:1.0 


apiVersion: apps/v1
kind: Deployment
metadata:
  name: bsp-deployment
spec:
  selector:
    matchLabels:
      app: bsp
  replicas: 1
  template:
    metadata:
      labels:
        app: bsp
    spec:
      containers:
      - name: bsp
        image: 192.168.111.1:5000/bsp:1.0
        env:
	    - name: SERVER_PORT
	      value: "80"
        ports:
        - containerPort: 80
        volumeMounts:
        - mountPath: "/root/logs/bsp/"
          name: bsp-log
      volumes:
      - name: bsp-log
        hostPath: 
          path: "C:/Users/pewee/logs/bsp/"

含义解释:
将宿主机 C:/Users/pewee/logs/bsp/ 挂载到容器/root/logs/bsp/
在pod中定义的volumes,其中的所有container都可以使用

#### initcontainer

	apiVersion: v1
	kind: Pod
	metadata:
	  name: javaweb-2
	spec:
	  initContainers:
	  - image: geektime/sample:v2
	    name: war
	    command: ["cp", "/sample.war", "/app"]
	    volumeMounts:
	    - mountPath: /app
	      name: app-volume
	  containers:
	  - image: geektime/tomcat:7.0
	    name: tomcat
	    command: ["sh","-c","/root/apache-tomcat-7.0.42-v2/bin/start.sh"]
	    volumeMounts:
	    - mountPath: /root/apache-tomcat-7.0.42-v2/webapps
	      name: app-volume
	    ports:
	    - containerPort: 8080
	      hostPort: 8001 
	  volumes:
	  - name: app-volume
	    emptyDir: {}

* 在 Pod 中，所有 Init Container 定义的容器，都会比 spec.containers 定义的用户容器先启动。并且，Init Container 容器会按顺序逐一启动，而直到它们都启动并且退出了，用户容器才会启动。所以，这个 Init Container 类型的 WAR 包容器启动后，我执行了一句"cp /sample.war /app"，把应用的 WAR 包拷贝到 /app 目录下，然后退出。而后这个 /app 目录，就挂载了一个名叫 app-volume 的 Volume。接下来就很关键了。
* Tomcat 容器，同样声明了挂载 app-volume 到自己的 webapps 目录下。所以，等 Tomcat 容器启动时，它的 webapps 目录下就一定会存在 sample.war 文件：这个文件正是 WAR 包容器启动时拷贝到这个 Volume 里面的，而这个 Volume 是被这两个容器共享的。

#### 容器的日志收集

比如，我现在有一个应用，需要不断地把日志文件输出到容器的 /var/log 目录中。这时，我就可以把一个 Pod 里的 Volume 挂载到应用容器的 /var/log 目录上。然后，我在这个 Pod 里同时运行一个 sidecar 容器，它也声明挂载同一个 Volume 到自己的 /var/log 目录上。这样，接下来 sidecar 容器就只需要做一件事儿，那就是不断地从自己的 /var/log 目录里读取日志文件，转发到 MongoDB 或者 Elasticsearch 中存储起来。这样，一个最基本的日志收集工作就完成了。\

### pod

* NodeSelector：是一个供用户将 Pod 与 Node 进行绑定的字段

	apiVersion: v1
	kind: Pod
	...
	spec:
	 nodeSelector:
	   disktype: ssd

这样的一个配置，意味着这个 Pod 永远只能运行在携带了“disktype: ssd”标签（Label）的节点上；否则，它将调度失败。

* HostAliases：定义了 Pod 的 hosts 文件（比如 /etc/hosts）里的内容


		apiVersion: v1
		kind: Pod
		...
		spec:
		  hostAliases:
		  - ip: "10.1.2.3"
		    hostnames:
		    - "foo.remote"
		    - "bar.remote"
		...
在这个 Pod 的 YAML 文件中，我设置了一组 IP 和 hostname 的数据。这样，这个 Pod 启动后，/etc/hosts 文件的内容将如下所示：

		cat /etc/hosts
		# Kubernetes-managed hosts file.
		127.0.0.1 localhost
		...
		10.244.135.10 hostaliases-pod
		10.1.2.3 foo.remote
		10.1.2.3 bar.remote
在 Kubernetes 项目中，如果要设置 hosts 文件里的内容，一定要通过这种方法。否则，如果直接修改了 hosts 文件的话，在 Pod 被删除重建之后，kubelet 会自动覆盖掉被修改的内容。

* Pod 对象在 Kubernetes 中的生命周期。

	  * Pending。这个状态意味着，Pod 的 YAML 文件已经提交给了 Kubernetes，API 对象已经被创建并保存在 Etcd 当中。但是，这个 Pod 里有些容器因为某种原因而不能被顺利创建。比如，调度不成功。
	  * Running。这个状态下，Pod 已经调度成功，跟一个具体的节点绑定。它包含的容器都已经创建成功，并且至少有一个正在运行中。
	  * Succeeded。这个状态意味着，Pod 里的所有容器都正常运行完毕，并且已经退出了。这种情况在运行一次性任务时最为常见。
	  * Failed。这个状态下，Pod 里至少有一个容器以不正常的状态（非 0 的返回码）退出。这个状态的出现，意味着你得想办法 Debug 这个容器的应用，比如查看 Pod 的 Events 和日志。
	  * Unknown。这是一个异常状态，意味着 Pod 的状态不能持续地被 kubelet 汇报给 kube-apiserver，这很有可能是主从节点（Master 和 Kubelet）间的通信出现了问题。

### Volume
1. Secret:
Secret 最典型的使用场景，莫过于存放数据库的 Credential 信息

		apiVersion: v1
		kind: Pod
		metadata:
		  name: test-projected-volume 
		spec:
		  containers:
		  - name: test-secret-volume
		    image: busybox
		    args:
		    - sleep
		    - "86400"
		    volumeMounts:
		    - name: mysql-cred
		      mountPath: "/projected-volume"
		      readOnly: true
		  volumes:
		  - name: mysql-cred
		    projected:
		      sources:
		      - secret:
		          name: user
		      - secret:
		          name: pass
这里我定义了一个简单的容器。它声明挂载的 Volume，并不是常见的 emptyDir 或者 hostPath 类型，而是 projected 类型。而这个 Volume 的数据来源（sources），则是名为 user 和 pass 的 Secret 对象，分别对应的是数据库的用户名和密码。
设置密码

		$ echo -n 'admin' | base64
		YWRtaW4=
		$ echo -n '1f2d1e2e67df' | base64
		MWYyZDFlMmU2N2Rm
通过编写 YAML 文件的方式来创建这个 Secret 对象

		apiVersion: v1
		kind: Secret
		metadata:
		  name: mysecret
		type: Opaque
		data:
		  user: YWRtaW4=
		  pass: MWYyZDFlMmU2N2Rm
创建这个 Pod：kubectl create -f test-projected-volume.yaml

		$ kubectl exec -it test-projected-volume -- /bin/sh
		$ ls /projected-volume/
		user
		pass
		$ cat /projected-volume/user
		root
		$ cat /projected-volume/pass
		1f2d1e2e67df

2. ConfigMap；
 * 与 Secret 的区别在于，ConfigMap 保存的是不需要加密的、应用所需的配置信息。而 ConfigMap 的用法几乎与 Secret 完全相同：你可以使用 kubectl create configmap 从文件或者目录创建 ConfigMap，也可以直接编写 ConfigMap 对象的 YAML 文件。
 * 一个 Java 应用所需的配置文件（.properties 文件），就可以通过下面这样的方式保存在 ConfigMap 里：
	
			# .properties文件的内容
			$ cat example/ui.properties
			color.good=purple
			color.bad=yellow
			allow.textmode=true
			how.nice.to.look=fairlyNice
				
			# 从.properties文件创建ConfigMap
			$ kubectl create configmap ui-config --from-file=example/ui.properties
				
			# 查看这个ConfigMap里保存的信息(data)
			$ kubectl get configmaps ui-config -o yaml
			apiVersion: v1
			data:
			  ui.properties: |
			    color.good=purple
				    color.bad=yellow
			    allow.textmode=true
			    how.nice.to.look=fairlyNice
			kind: ConfigMap
			metadata:
			  name: ui-config
			  ...

3. Downward API:Pod 里的容器能够直接获取到这个 Pod API 对象本身的信息。
例子:

		apiVersion: v1
		kind: Pod
		metadata:
		  name: test-downwardapi-volume
		  labels:
		    zone: us-est-coast
		    cluster: test-cluster1
		    rack: rack-22
		spec:
		  containers:
		    - name: client-container
		      image: k8s.gcr.io/busybox
		      command: ["sh", "-c"]
		      args:
		      - while true; do
		          if [[ -e /etc/podinfo/labels ]]; then
		            echo -en '\n\n'; cat /etc/podinfo/labels; fi;
		          sleep 5;
		        done;
		      volumeMounts:
		        - name: podinfo
		          mountPath: /etc/podinfo
		          readOnly: false
		  volumes:
		    - name: podinfo
		      projected:
		        sources:
		        - downwardAPI:
		            items:
		              - path: "labels"
		                fieldRef:
		                  fieldPath: metadata.labels
这里把pod的metadata.labels的内容作为labels文件在container中挂载到了目录下读取:

		$ kubectl create -f dapi-volume.yaml
		$ kubectl logs test-downwardapi-volume
		cluster="test-cluster1"
		rack="rack-22"
		zone="us-est-coast"

		1.使用fieldRef可以声明使用:
		spec.nodeName - 宿主机名字
		status.hostIP - 宿主机IP
		metadata.name - Pod的名字
		metadata.namespace - Pod的Namespace
		status.podIP - Pod的IP
		spec.serviceAccountName - Pod的Service Account的名字
		metadata.uid - Pod的UID
		metadata.labels['<KEY>'] - 指定<KEY>的Label值
		metadata.annotations['<KEY>'] - 指定<KEY>的Annotation值
		metadata.labels - Pod的所有Label
		metadata.annotations - Pod的所有Annotation
		
		2. 使用resourceFieldRef可以声明使用:
		容器的CPU limit
		容器的CPU request
		容器的memory limit
		容器的memory request

4. ServiceAccountToken :
    >任何运行在 Kubernetes 集群上的应用，都必须使用这个 ServiceAccountToken 里保存的授权信息，也就是 Token，才可以合法地访问 API Server,为了方便使用，Kubernetes 已经为你提供了一个默认“服务账户”（default Service Account）。并且，任何一个运行在 Kubernetes 里的 Pod，都可以直接使用这个默认的 Service Account，而无需显示地声明挂载它。

		$ kubectl describe pod nginx-deployment-5c678cfb6d-lg9lw
		Containers:
		...
		  Mounts:
		    /var/run/secrets/kubernetes.io/serviceaccount from default-token-s8rbq (ro)
		Volumes:
		  default-token-s8rbq:
		  Type:       Secret (a volume populated by a Secret)
		  SecretName:  default-token-s8rbq
		  Optional:    false
   一旦 Pod 创建完成，容器里的应用就可以直接从这个默认 ServiceAccountToken 的挂载目录里访问到授权信息和文件。这个容器内的路径在 Kubernetes 里是固定的，即：/var/run/secrets/kubernetes.io/serviceaccount

### 探针Probe
>在 Kubernetes 中，你可以为 Pod 里的容器定义一个健康检查“探针”（Probe）。这样，kubelet 就会根据这个 Probe 的返回值决定这个容器的状态，而不是直接以容器镜像是否运行（来自 Docker 返回的信息）作为依据。这种机制，是生产环境中保证应用健康存活的重要手段。
	
	apiVersion: v1
	kind: Pod
	metadata:
	  labels:
	    test: liveness
	  name: test-liveness-exec
	spec:
	  containers:
	  - name: liveness
	    image: busybox
	    args:
	    - /bin/sh
	    - -c
	    - touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600
	    livenessProbe:
	      exec:
	        command:
	        - cat
	        - /tmp/healthy
	      initialDelaySeconds: 5
	      periodSeconds: 5
在这个 Pod 中，我们定义了一个有趣的容器。它在启动之后做的第一件事，就是在 /tmp 目录下创建了一个 healthy 文件，以此作为自己已经正常运行的标志。而 30 s 过后，它会把这个文件删除掉。与此同时，我们定义了一个这样的 livenessProbe（健康检查）。它的类型是 exec，这意味着，它会在容器启动后，在容器里面执行一条我们指定的命令，比如：“cat /tmp/healthy”。这时，如果这个文件存在，这条命令的返回值就是 0，Pod 就会认为这个容器不仅已经启动，而且是健康的。这个健康检查，在容器启动 5 s 后开始执行（initialDelaySeconds: 5），每 5 s 执行一次（periodSeconds: 5）。

kubectl create -f test-liveness-exec.yaml
kubectl get pod
kubectl describe pod test-liveness-exec
发现，这个 Pod 在 Events 报告了一个异常：

显然，这个健康检查探查到 /tmp/healthy 已经不存在了，所以它报告容器是不健康的。那么接下来会发生什么呢？

我们不妨再次查看一下这个 Pod 的状态：发现，Pod 并没有进入 Failed 状态，而是保持了 Running 状态
RESTARTS 字段从 0 到 1 的变化，就明白原因了：这个异常的容器已经被 Kubernetes 重启了

Pod 的恢复过程，永远都是发生在当前节点上，而不会跑到别的节点上去。事实上，一旦一个 Pod 与一个节点（Node）绑定，除非这个绑定发生了变化（pod.spec.node 字段被修改），否则它永远都不会离开这个节点。这也就意味着，如果这个宿主机宕机了，这个 Pod 也不会主动迁移到其他节点上去。而如果你

想让 Pod 出现在其他的可用节点上，就必须使用 Deployment 这样的“控制器”来管理 Pod，哪怕你只需要一个 Pod 副本。	

	...
	livenessProbe:
	     httpGet:
	       path: /healthz
	       port: 8080
	       httpHeaders:
	       - name: X-Custom-Header
	         value: Awesome
	       initialDelaySeconds: 3
	       periodSeconds: 3

### PodPreset


开发人员编写简单的编排

	apiVersion: v1
	kind: Pod
	metadata:
	  name: website
	  labels:
	    app: website
	    role: frontend
	spec:
	  containers:
	    - name: website
	      image: nginx
	      ports:
	        - containerPort: 80
运维人员定义好模板:preset.yaml

	apiVersion: settings.k8s.io/v1alpha1
	kind: PodPreset
	metadata:
	  name: allow-database
	spec:
	  selector:
	    matchLabels:
	      role: frontend
	  env:
	    - name: DB_PORT
	      value: "6379"
	  volumeMounts:
	    - mountPath: /cache
	      name: cache-volume
	  volumes:
	    - name: cache-volume
	      emptyDir: {}
运维人员先创建了这个 PodPreset，然后开发人员才创建 Pod：

	$ kubectl get pod website -o yaml
	apiVersion: v1
	kind: Pod
	metadata:
	  name: website
	  labels:
	    app: website
	    role: frontend
	  annotations:
	    podpreset.admission.kubernetes.io/podpreset-allow-database: "resource version"
	spec:
	  containers:
	    - name: website
	      image: nginx
	      volumeMounts:
	        - mountPath: /cache
	          name: cache-volume
	      ports:
	        - containerPort: 80
	      env:
	        - name: DB_PORT
	          value: "6379"
	  volumes:
	    - name: cache-volume
	      emptyDir: {}

### ReplicaSet

	apiVersion: apps/v1
	kind: ReplicaSet
	metadata:
	  name: nginx-set
	  labels:
	    app: nginx
	spec:
	  replicas: 3
	  selector:
	    matchLabels:
	      app: nginx
	  template:
	    metadata:
	      labels:
	        app: nginx
	    spec:
	      containers:
	      - name: nginx
	        image: nginx:1.7.9
一个 ReplicaSet 对象，其实就是由副本数目的定义和一个 Pod 模板组成的。不难发现，它的定义其实是 Deployment 的一个子集。更重要的是，Deployment 控制器实际操纵的，正是这样的 ReplicaSet 对象，而不是 Pod 对象。	
![ReplicaSet和Deployment](k8s/1.jpg)

* Deployment的扩展与收缩

		$ kubectl scale deployment nginx-deployment --replicas=4
		deployment.apps/nginx-deployment scaled
* Deployment的回滚

		$ kubectl rollout undo deployment/nginx-deployment
		deployment.extensions/nginx-deployment
		回滚到更早之前的版本
		首先，我需要使用 kubectl rollout history deployment/nginx-deployment命令，查看每次 Deployment 变更对应的版本。
		$ kubectl rollout history deployment/nginx-deployment --revision=2
		然后，我们就可以在 kubectl rollout undo 命令行最后，加上要回滚到的指定版本的版本号，就可以回滚到指定版本了。这个指令的用法如下：
		$ kubectl rollout undo deployment/nginx-deployment --to-revision=2
		deployment.extensions/nginx-deployment
* spec.revisionHistoryLimit=0,控制deployment的历史版本数量,这样就不能回滚了

***

### statefulset和Headless Service:处理有状态应用场景,应用之间存在依赖关系
1. 应用的拓扑依赖
2. 应用的存储状态:如部署的DB实例在POD重启后读取的是同一个文件内容

StatefulSet 的核心功能，就是通过某种方式记录这些状态，然后在 Pod 被重新创建时，能够为新 Pod 恢复这些状态。
#### Service:Kubernetes 项目中用来将一组 Pod 暴露给外界访问的一种机制
访问方式:

1. 是以 Service 的 VIP（Virtual IP，即：虚拟 IP）方式
当我访问 10.0.23.1 这个 Service 的 IP 地址时，10.0.23.1 其实就是一个 VIP，它会把请求转发到该 Service 所代理的某一个 Pod 上
2. 就是以 Service 的 DNS 方式:
只要我访问“my-svc.my-namespace.svc.cluster.local”这条 DNS 记录，就可以访问到名叫 my-svc 的 Service 所代理的某一个 Pod。
	* Normal Service。这种情况下，你访问“my-svc.my-namespace.svc.cluster.local”解析到的，正是 my-svc 这个 Service 的 VIP，后面的流程就跟 VIP 方式一致了。
	* Headless Service。这种情况下，你访问“my-svc.my-namespace.svc.cluster.local”解析到的，直接就是 my-svc 代理的某一个 Pod 的 IP 地址。可以看到，这里的区别在于，Headless Service 不需要分配一个 VIP，而是可以直接以 DNS 记录的方式解析出被代理 Pod 的 IP 地址。

Headless Service示例:

	apiVersion: v1
	kind: Service
	metadata:
	  name: bsp-service
	  labels:
	    app: bsp-service
	spec:
	  ports:
	  - port: 80
	    name: bsp-web
	  clusterIP: None
	  selector:
	    app: bsp
所谓的 Headless Service，其实仍是一个标准 Service 的 YAML 文件。只不过，它的 clusterIP 字段的值是：None，即：这个 Service，没有一个 VIP 作为“头”。这也就是 Headless 的含义。所以，这个 Service 被创建后并不会被分配一个 VIP，而是会以 DNS 记录的方式暴露出它所代理的 Pod。
当你按照这样的方式创建了一个 Headless Service 之后，它所代理的所有 Pod 的 IP 地址，都会被绑定一个这样格式的 DNS 记录，如下所示：

	<pod-name>.<svc-name>.<namespace>.svc.cluster.local
这个 DNS 记录，正是 Kubernetes 项目为 Pod 分配的唯一的“可解析身份”（Resolvable Identity）。有了这个“可解析身份”，只要你知道了一个 Pod 的名字，以及它对应的 Service 的名字，你就可以非常确定地通过这条 DNS 记录访问到 Pod 的 IP 地址。

StatefulSet示例文件:

	apiVersion: apps/v1
	kind: StatefulSet
	metadata:
	  name: bsp-web
	spec:
	  serviceName: "bsp-service"
	  replicas: 1
	  selector:
	    matchLabels:
	      app: bsp
	  template:
	    metadata:
	      labels:
	        app: bsp
	    spec:
	      containers:
	      - name: bsp
	        image: 192.168.111.1:5000/bsp:1.0
	        env:
	        - name: SERVER_PORT
	          value: "80"
	        ports:
			  name: bsp-web
	        - containerPort: 80
	        volumeMounts:
	        - mountPath: "/root/logs/bsp/"
	          name: bsp-log
	      volumes:
	      - name: bsp-log
	        hostPath:
	          path: "/root/logs/bsp/"
这个 YAML 文件，和deployment的yml 的唯一区别，就是多了一个 serviceName=XXXX 字段。
这个字段的作用，就是告诉 StatefulSet 控制器，在执行控制循环（Control Loop）的时候，请使用 XXXX 这个 Headless Service 来保证 Pod 的“可解析身份”。
通过启动service和statefulset,之后查看状态,看到创建了以
> statefulset name-ordinal index 命名的pod,这些编号都是从 0 开始累加，与 StatefulSet 的每个 Pod 实例一一对应

	[root@k8s-master-node1 ~]#kubectl apply -f bsp-service.yml 
	[root@k8s-master-node1 ~]#kubectl apply -f bsp-statefulset.yml 
	[root@k8s-master-node1 ~]# kubectl get service bsp-service
	NAME          TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
	bsp-service   ClusterIP   None         <none>        80/TCP    3m42s
	[root@k8s-master-node1 ~]# kubectl get statefulset
	NAME      READY   AGE
	bsp-web   1/1     81s
	[root@k8s-master-node1 ~]# kubectl get pods -w -l app=bsp
	NAME        READY   STATUS    RESTARTS   AGE
	bsp-web-0   1/1     Running   0          110s
	[root@k8s-master-node1 ~]# kubectl exec bsp-web-0 -- sh -c 'hostname'
	bsp-web-0

通过busybox查看ip:

	kubectl run -i --tty --image busybox:1.28.4 dns-test --restart=Never --rm /bin/sh
	通过这条命令，我们启动了一个一次性的 Pod，因为 --rm 意味着 Pod 退出后就会被删除掉。然后，在这个 Pod 的容器里面，我们尝试用 nslookup 命令，解析一下 Pod 对应的 Headless Service：
	# nslookup bsp-web-0.bsp-service
	Server:    10.96.0.10
	Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local
	[root@k8s-master-node1 ~]#ping bsp-web-0.bsp-service.default.svc.cluster.local
Pod 的 IP 地址，并不是固定的。这就意味着，对于“有状态应用”实例的访问，你必须使用 DNS 记录或者 hostname 的方式，而绝不应该直接访问这些 Pod 的 IP 地址。
#### StatefulSet 之存储状态
第一步：定义一个 PVC，声明想要的 Volume 的属性：

	kind: PersistentVolumeClaim
	apiVersion: v1
	metadata:
	  name: pv-claim
	spec:
	  accessModes:
	  - ReadWriteOnce
	  resources:
	    requests:
	      storage: 1Gi
第二步：在应用的 Pod 中，声明使用这个 PVC：

	apiVersion: v1
	kind: Pod
	metadata:
	  name: pv-pod
	spec:
	  containers:
	    - name: pv-container
	      image: nginx
	      ports:
	        - containerPort: 80
	          name: "http-server"
	      volumeMounts:
	        - mountPath: "/usr/share/nginx/html"
	          name: pv-storage
	  volumes:
	    - name: pv-storage
	      persistentVolumeClaim:
	        claimName: pv-claim
可以看到，在这个 Pod 的 Volumes 定义中，我们只需要声明它的类型是 persistentVolumeClaim，然后指定 PVC 的名字，而完全不必关心 Volume 本身的定义。可是，这些符合条件的 Volume 又是从哪里来的呢？

答案是，它们来自于由运维人员维护的 PV（Persistent Volume）对象。接下来，我们一起看一个常见的 PV 对象的 YAML 文件：

	kind: PersistentVolume
	apiVersion: v1
	metadata:
	  name: pv-volume
	  labels:
	    type: local
	spec:
	  capacity:
	    storage: 10Gi
	  accessModes:
	    - ReadWriteOnce
	  rbd:
	    monitors:
	    # 使用 kubectl get pods -n rook-ceph 查看 rook-ceph-mon- 开头的 POD IP 即可得下面的列表
	    - '10.16.154.78:6789'
	    - '10.16.154.82:6789'
	    - '10.16.154.83:6789'
	    pool: kube
	    image: foo
	    fsType: ext4
	    readOnly: true
	    user: admin
	    keyring: /etc/ceph/keyring
所以，Kubernetes 中 PVC 和 PV 的设计，实际上类似于“接口”和“实现”的思想。开发者只要知道并会使用“接口”，即：PVC；而运维人员则负责给“接口”绑定具体的实现，即：PV。这种解耦，就避免了因为向开发者暴露过多的存储系统细节而带来的隐患。此外，这种职责的分离，往往也意味着出现事故时可以更容易定位问题和明确责任，从而避免“扯皮”现象的出现。

	apiVersion: apps/v1
	kind: StatefulSet
	metadata:
	  name: web
	spec:
	  serviceName: "nginx"
	  replicas: 2
	  selector:
	    matchLabels:
	      app: nginx
	  template:
	    metadata:
	      labels:
	        app: nginx
	    spec:
	      containers:
	      - name: nginx
	        image: nginx:1.9.1
	        ports:
	        - containerPort: 80
	          name: web
	        volumeMounts:
	        - name: www
	          mountPath: /usr/share/nginx/html
	  volumeClaimTemplates:
	  - metadata:
	      name: www
	    spec:
	      accessModes:
	      - ReadWriteOnce
	      resources:
	        requests:
	          storage: 1Gi

这次，我们为这个 StatefulSet 额外添加了一个 volumeClaimTemplates 字段。从名字就可以看出来，它跟 Deployment 里 Pod 模板（PodTemplate）的作用类似。也就是说，凡是被这个 StatefulSet 管理的 Pod，都会声明一个对应的 PVC；而这个 PVC 的定义，就来自于 volumeClaimTemplates 这个模板字段。更重要的是，这个 PVC 的名字，会被分配一个与这个 Pod 完全一致的编号。

这个自动创建的 PVC，与 PV 绑定成功后，就会进入 Bound 状态，这就意味着这个 Pod 可以挂载并使用这个 PV 了。

如果你还是不太理解 PVC 的话，可以先记住这样一个结论：PVC 其实就是一种特殊的 Volume。只不过一个 PVC 具体是什么类型的 Volume，要在跟某个 PV 绑定之后才知道。关于 PV、PVC 更详细的知识，我会在容器存储部分做进一步解读。

当然，PVC 与 PV 的绑定得以实现的前提是，运维人员已经在系统里创建好了符合条件的 PV（比如，我们在前面用到的 pv-volume）；或者，你的 Kubernetes 集群运行在公有云上，这样 Kubernetes 就会通过 Dynamic Provisioning 的方式，自动为你创建与 PVC 匹配的 PV。

所以，我们在使用 kubectl create 创建了 StatefulSet 之后，就会看到 Kubernetes 集群里出现了两个 PVC：

	$ kubectl create -f statefulset.yaml
	$ kubectl get pvc -l app=nginx
	NAME        STATUS    VOLUME                                     CAPACITY   ACCESSMODES   AGE
	www-web-0   Bound     pvc-15c268c7-b507-11e6-932f-42010a800002   1Gi        RWO           48s
	www-web-1   Bound     pvc-15c79307-b507-11e6-932f-42010a800002   1Gi        RWO           48s

可以看到，这些 PVC，都以PVC 名字 -StatefulSet 名字 - 编号 的方式命名，并且处于 Bound 状态。我们前面已经讲到过，这个 StatefulSet 创建出来的所有 Pod，都会声明使用编号的 PVC。比如，在名叫 web-0 的 Pod 的 volumes 字段，它会声明使用名叫 www-web-0 的 PVC，从而挂载到这个 PVC 所绑定的 PV。

首先，当你把一个 Pod，比如 web-0，删除之后，这个 Pod 对应的 PVC 和 PV，并不会被删除，而这个 Volume 里已经写入的数据，也依然会保存在远程存储服务里（比如，我们在这个例子里用到的 Ceph 服务器）。此时，StatefulSet 控制器发现，一个名叫 web-0 的 Pod 消失了。所以，控制器就会重新创建一个新的、名字还是叫作 web-0 的 Pod 来，“纠正”这个不一致的情况。需要注意的是，在这个新的 Pod 对象的定义里，它声明使用的 PVC 的名字，还是叫作：www-web-0。这个 PVC 的定义，还是来自于 PVC 模板（volumeClaimTemplates），这是 StatefulSet 创建 Pod 的标准流程。所以，在这个新的 web-0 Pod 被创建出来之后，Kubernetes 为它查找名叫 www-web-0 的 PVC 时，就会直接找到旧 Pod 遗留下来的同名的 PVC，进而找到跟这个 PVC 绑定在一起的 PV。这样，新的 Pod 就可以挂载到旧 Pod 对应的那个 Volume，并且获取到保存在 Volume 里的数据。通过这种方式，Kubernetes 的 StatefulSet 就实现了对应用存储状态的管理。

***
### DaemonSet
1. 这个 Pod 运行在 Kubernetes 集群里的每一个节点（Node）上；
2. 每个节点上只有一个这样的 Pod 实例；
3. 当有新的节点加入 Kubernetes 集群后，该 Pod 会自动地在新节点上被创建出来；而当旧节点被删除后，它上面的 Pod 也相应地会被回收掉。


		apiVersion: apps/v1
		kind: DaemonSet
		metadata:
		  name: fluentd-elasticsearch
		  namespace: kube-system
		  labels:
		    k8s-app: fluentd-logging
		spec:
		  selector:
		    matchLabels:
		      name: fluentd-elasticsearch
		  template:
		    metadata:
		      labels:
		        name: fluentd-elasticsearch
		    spec:
		      tolerations:
		      - key: node-role.kubernetes.io/master
		        effect: NoSchedule
		      containers:
		      - name: fluentd-elasticsearch
		        image: k8s.gcr.io/fluentd-elasticsearch:1.20
		        resources:
		          limits:
		            memory: 200Mi
		          requests:
		            cpu: 100m
		            memory: 200Mi
		        volumeMounts:
		        - name: varlog
		          mountPath: /var/log
		        - name: varlibdockercontainers
		          mountPath: /var/lib/docker/containers
		          readOnly: true
		      terminationGracePeriodSeconds: 30
		      volumes:
		      - name: varlog
		        hostPath:
		          path: /var/log
		      - name: varlibdockercontainers
		        hostPath:
		          path: /var/lib/docker/containers
DaemonSet 跟 Deployment 其实非常相似，只不过是没有 replicas 字段；它也使用 selector 选择管理所有携带了 name=fluentd-elasticsearch 标签的 Pod。

我们定义了一个使用 fluentd-elasticsearch:1.20 镜像的容器，而且这个容器挂载了两个 hostPath 类型的 Volume，分别对应宿主机的 /var/log 目录和 /var/lib/docker/containers 目录。

显然，fluentd 启动之后，它会从这两个目录里搜集日志信息，并转发给 ElasticSearch 保存。这样，我们通过 ElasticSearch 就可以很方便地检索这些日志了。

需要注意的是，Docker 容器里应用的日志，默认会保存在宿主机的 /var/lib/docker/containers/{{. 容器 ID}}/{{. 容器 ID}}-json.log 文件里，所以这个目录正是 fluentd 的搜集目标。

	apiVersion: v1
	kind: Pod
	metadata:
	  name: with-node-affinity
	spec:
	  affinity:
	    nodeAffinity:
	      requiredDuringSchedulingIgnoredDuringExecution:
	        nodeSelectorTerms:
	        - matchExpressions:
	          - key: metadata.name
	            operator: In
	            values:
	            - node-geektime

在pod中指定nodeAffinity表示pod可以在哪个node上调度

1. requiredDuringSchedulingIgnoredDuringExecution：它的意思是说，这个 nodeAffinity 必须在每次调度的时候予以考虑。同时，这也意味着你可以设置在某些情况下不考虑这个 nodeAffinity；
2. 这个 Pod，将来只允许运行在“metadata.name”是“node-geektime”的节点上。

污点容忍

	apiVersion: v1
	kind: Pod
	metadata:
	  name: with-toleration
	spec:
	  tolerations:
	  - key: node.kubernetes.io/unschedulable
	    operator: Exists
	    effect: NoSchedule
“容忍”所有被标记为 unschedulable“污点”的 Node；“容忍”的效果是允许调度。
需要注意的是，在 DaemonSet 上，我们一般都应该加上 resources 字段，来限制它的 CPU 和内存使用，防止它占用过多的宿主机资源。

***
### Job与CronJob

	apiVersion: batch/v1
	kind: Job
	metadata:
	  name: pi
	spec:
	  template:
	    spec:
	      containers:
	      - name: pi
	        image: resouer/ubuntu-bc 
	        command: ["sh", "-c", "echo 'scale=10000; 4*a(1)' | bc -l "]
	      restartPolicy: Never
	  backoffLimit: 4
跟其他控制器不同的是，Job 对象并不要求你定义一个 spec.selector 来描述要控制哪些 Pod。

定义了 restartPolicy=Never，那么离线作业失败后 Job Controller 就会不断地尝试创建一个新 Pod

当然，这个尝试肯定不能无限进行下去。所以，我们就在 Job 对象的 spec.backoffLimit 字段里定义了重试次数为 4（即，backoffLimit=4），而这个字段的默认值是 6。

而如果你定义的 restartPolicy=OnFailure，那么离线作业失败后，Job Controller 就不会去尝试创建新的 Pod。但是，它会不断地尝试重启 Pod 里的容器。

当一个 Job 的 Pod 运行结束后，它会进入 Completed 状态。但是，如果这个 Pod 因为某种原因一直不肯结束呢？在 Job 的 API 对象里，有一个 spec.activeDeadlineSeconds 字段可以设置最长运行时间，比如：
	
	spec:
	 backoffLimit: 5
	 activeDeadlineSeconds: 100
Job Controller 对并行作业的控制方法。

1. spec.parallelism，它定义的是一个 Job 在任意时间最多可以启动多少个 Pod 同时运行；
2. spec.completions，它定义的是 Job 至少要完成的 Pod 数目，即 Job 的最小完成数。

		apiVersion: batch/v1
		kind: Job
		metadata:
		  name: pi
		spec:
		  parallelism: 2
		  completions: 4
		  template:
		    spec:
		      containers:
		      - name: pi
		        image: resouer/ubuntu-bc
		        command: ["sh", "-c", "echo 'scale=5000; 4*a(1)' | bc -l "]
		      restartPolicy: Never
		  backoffLimit: 4

CronJob

		apiVersion: batch/v1beta1
		kind: CronJob
		metadata:
		  name: hello
		spec:
		  schedule: "*/1 * * * *"
		  jobTemplate:
		    spec:
		      template:
		        spec:
		          containers:
		          - name: hello
		            image: busybox
		            args:
		            - /bin/sh
		            - -c
		            - date; echo Hello from the Kubernetes cluster
		          restartPolicy: OnFailure
CronJob 与 Job 的关系，正如同 Deployment 与 ReplicaSet 的关系一样。CronJob 是一个专门用来管理 Job 对象的控制器。只不过，它创建和删除 Job 的依据，是 schedule 字段定义的、一个标准的Unix Cron格式的表达式。
由于定时任务的特殊性，很可能某个 Job 还没有执行完，另外一个新 Job 就产生了。这时候，你可以通过 spec.concurrencyPolicy 字段来定义具体的处理策略。比如：

1. concurrencyPolicy=Allow，这也是默认情况，这意味着这些 Job 可以同时存在；
2. concurrencyPolicy=Forbid，这意味着不会创建新的 Pod，该创建周期被跳过；
3. concurrencyPolicy=Replace，这意味着新产生的 Job 会替换旧的、没有执行完的 Job。

***
### Dynamic Admission Control
在 Kubernetes 项目中，当一个 Pod 或者任何一个 API 对象被提交给 APIServer 之后，总有一些“初始化”性质的工作需要在它们被 Kubernetes 项目正式处理之前进行。

比如，自动为所有 Pod 加上某些标签（Labels）。而这个“初始化”操作的实现，借助的是一个叫作 Admission 的功能。它其实是 Kubernetes 项目里一组被称为 Admission Controller 的代码，可以选择性地被编译进 APIServer 中，在 API 对象创建之后会被立刻调用到。

但这就意味着，如果你现在想要添加一些自己的规则到 Admission Controller，就会比较困难。因为，这要求重新编译并重启 APIServer。显然，这种使用方法对 Istio 来说，影响太大了。

所以，Kubernetes 项目为我们额外提供了一种“热插拔”式的 Admission 机制，它就是 Dynamic Admission Control，也叫作：Initializer。

如下所示的一个应用 Pod：

	apiVersion: v1
	kind: Pod
	metadata:
	  name: myapp-pod
	  labels:
	    app: myapp
	spec:
	  containers:
	  - name: myapp-container
	    image: busybox
	    command: ['sh', '-c', 'echo Hello Kubernetes! && sleep 3600']

Istio 项目要做的，就是在这个 Pod YAML 被提交给 Kubernetes 之后，在它对应的 API 对象里自动加上 Envoy 容器的配置，使这个对象变成如下所示的样子：

	apiVersion: v1
	kind: Pod
	metadata:
	  name: myapp-pod
	  labels:
	    app: myapp
	spec:
	  containers:
	  - name: myapp-container
	    image: busybox
	    command: ['sh', '-c', 'echo Hello Kubernetes! && sleep 3600']
	  - name: envoy
	    image: lyft/envoy:845747b88f102c0fd262ab234308e9e22f693a1
	    command: ["/usr/local/bin/envoy"]
   	 	...

Istio 会将这个 Envoy 容器本身的定义，以 ConfigMap 的方式保存在 Kubernetes 当中。这个 ConfigMap（名叫：envoy-initializer）的定义如下所示：

	apiVersion: v1
	kind: ConfigMap
	metadata:
	  name: envoy-initializer
	data:
	  config: |
	    containers:
	      - name: envoy
	        image: lyft/envoy:845747db88f102c0fd262ab234308e9e22f693a1
	        command: ["/usr/local/bin/envoy"]
	        args:
	          - "--concurrency 4"
	          - "--config-path /etc/envoy/envoy.json"
	          - "--mode serve"
	        ports:
	          - containerPort: 80
	            protocol: TCP
	        resources:
	          limits:
	            cpu: "1000m"
	            memory: "512Mi"
	          requests:
	            cpu: "100m"
	            memory: "64Mi"
	        volumeMounts:
	          - name: envoy-conf
	            mountPath: /etc/envoy
	    volumes:
	      - name: envoy-conf
	        configMap:
	          name: envoy
Initializer 要做的工作，就是把这部分 Envoy 相关的字段，自动添加到用户提交的 Pod 的 API 对象里。可是，用户提交的 Pod 里本来就有 containers 字段和 volumes 字段，所以 Kubernetes 在处理这样的更新请求时，就必须使用类似于 git merge 这样的操作，才能将这两部分内容合并在一起。

在 Initializer 更新用户的 Pod 对象的时候，必须使用 PATCH API 来完成。而这种 PATCH API，正是声明式 API 最主要的能力。

接下来，Istio 将一个编写好的 Initializer，作为一个 Pod 部署在 Kubernetes 中。这个 Pod 的定义非常简单，如下所示：


	apiVersion: v1
	kind: Pod
	metadata:
	  labels:
	    app: envoy-initializer
	  name: envoy-initializer
	spec:
	  containers:
	    - name: envoy-initializer
	      image: envoy-initializer:0.0.1
	      imagePullPolicy: Always

Kubernetes 的控制器，实际上就是一个“死循环”：它不断地获取“实际状态”，然后与“期望状态”作对比，并以此为依据决定下一步的操作。

而 Initializer 的控制器，不断获取到的“实际状态”，就是用户新创建的 Pod。而它的“期望状态”，则是：这个 Pod 里被添加了 Envoy 容器的定义。

我还是用一段 Go 语言风格的伪代码，来为你描述这个控制逻辑，如下所示：

	for {
	  // 获取新创建的Pod
	  pod := client.GetLatestPod()
	  // Diff一下，检查是否已经初始化过
	  if !isInitialized(pod) {
	    // 没有？那就来初始化一下
	    doSomething(pod)
	  }
	}
Initializer 控制器的工作逻辑:

1. 从 APIServer 中拿到这个Envoy 容器定义的 ConfigMap
2. 把这个 ConfigMap 里存储的 containers 和 volumes 字段，直接添加进一个空的 Pod 对象里
3. Kubernetes 的 API 库，为我们提供了一个方法，使得我们可以直接使用新旧两个 Pod 对象，生成一个 TwoWayMergePatch,有了这个 TwoWayMergePatch 之后，Initializer 的代码就可以使用这个 patch 的数据，调用 Kubernetes 的 Client，发起一个 PATCH 请求
4. 每一个新创建的 Pod，都会自动携带了 metadata.initializers.pending 的 Metadata 信息,这个 Metadata，正是接下来 Initializer 的控制器判断这个 Pod 有没有执行过自己所负责的初始化操作的重要依据（也就是前面伪代码中 isInitialized() 方法的含义）.当你在 Initializer 里完成了要做的操作后，一定要记得将这个 metadata.initializers.pending 标志清除掉。这一点，你在编写 Initializer 代码的时候一定要非常注意

Istio 项目的核心，就是由无数个运行在应用 Pod 中的 Envoy 容器组成的服务代理网格。这也正是 Service Mesh 的含义。

而这个机制得以实现的原理，正是借助了 Kubernetes 能够对 API 对象进行在线更新的能力，这也正是 Kubernetes“声明式 API”的独特之处：

1. 首先，所谓“声明式”，指的就是我只需要提交一个定义好的 API 对象来“声明”，我所期望的状态是什么样子。
2. 其次，“声明式 API”允许有多个 API 写端，以 PATCH 的方式对 API 对象进行修改，而无需关心本地原始 YAML 文件的内容。
3. 最后，也是最重要的，有了上述两个能力，Kubernetes 项目才可以基于对 API 对象的增、删、改、查，在完全无需外界干预的情况下，完成对“实际状态”和“期望状态”的调谐（Reconcile）过程。

***
### k8s API
在 Kubernetes 项目中，一个 API 对象在 Etcd 里的完整资源路径，是由：Group（API 组）、Version（API 版本）和 Resource（API 资源类型）三个部分组成的。

![API](k8s/2.jpg)


	apiVersion: batch/v2alpha1
	kind: CronJob
	...
在这个 YAML 文件中，“CronJob”就是这个 API 对象的资源类型（Resource），“batch”就是它的组（Group），v2alpha1 就是它的版本（Version）。

Kubernetes 里的核心 API 对象，比如：Pod、Node 等，是不需要 Group 的（即：它们的 Group 是“”）。所以，对于这些 API 对象来说，Kubernetes 会直接在 /api 这个层级进行下一步的匹配过程。

* CRD:Custom Resource Definition:定义自定义 API 资源
* Custom controller:实现K8s API对义自定义 API 资源的CRUD操作

***
## PV,PVC,StorageClass
PV 描述的，是持久化存储数据卷。这个 API 对象主要定义的是一个持久化存储在宿主机上的目录，比如一个 NFS 的挂载目录。

通常情况下，PV 对象是由运维人员事先创建在 Kubernetes 集群里待用的。比如，运维人员可以定义这样一个 NFS 类型的 PV，如下所示：

	apiVersion: v1
	kind: PersistentVolume
	metadata:
	  name: nfs
	spec:
	  storageClassName: manual
	  capacity:
	    storage: 1Gi
	  accessModes:
	    - ReadWriteMany
	  nfs:
	    server: 10.244.1.4
	    path: "/"
PVC 描述的，则是 Pod 所希望使用的持久化存储的属性。比如，Volume 存储的大小、可读写权限等等。

比如，开发人员可以声明一个 1 GiB 大小的 PVC，如下所示：

	apiVersion: v1
	kind: PersistentVolumeClaim
	metadata:
	  name: nfs
	spec:
	  accessModes:
	    - ReadWriteMany
	  storageClassName: manual
	  resources:
	    requests:
	      storage: 1Gi
用户创建的 PVC 要真正被容器使用起来，就必须先和某个符合条件的 PV 进行绑定。这里要检查的条件，包括两部分：

1. 当然是 PV 和 PVC 的 spec 字段。比如，PV 的存储（storage）大小，就必须满足 PVC 的要求。
2. PV 和 PVC 的 storageClassName 字段必须一样。


在成功地将 PVC 和 PV 进行绑定之后，Pod 就能够像使用 hostPath 等常规类型的 Volume 一样，在自己的 YAML 文件里声明使用这个 PVC 了，如下所示
		

	apiVersion: v1
	kind: Pod
	metadata:
	  labels:
	    role: web-frontend
	spec:
	  containers:
	  - name: web
	    image: nginx
	    ports:
	      - name: web
	        containerPort: 80
	    volumeMounts:
	        - name: nfs
	          mountPath: "/usr/share/nginx/html"
	  volumes:
	  - name: nfs
	    persistentVolumeClaim:
	      claimName: nfs

Kubernetes 中，实际上存在着一个专门处理持久化存储的控制器，叫作 Volume Controller。这个 Volume Controller 维护着多个控制循环，其中有一个循环，扮演的就是撮合 PV 和 PVC 的“红娘”的角色。

它的名字叫作 PersistentVolumeController。PersistentVolumeController 会不断地查看当前每一个 PVC，是不是已经处于 Bound（已绑定）状态。如果不是，那它就会遍历所有的、可用的 PV，并尝试将其与这个“单身”的 PVC 进行绑定。

这样，Kubernetes 就可以保证用户提交的每一个 PVC，只要有合适的 PV 出现，它就能够很快进入绑定状态，从而结束“单身”之旅。

而所谓将一个 PV 与 PVC 进行“绑定”，其实就是将这个 PV 对象的名字，填在了 PVC 对象的 spec.volumeName 字段上。所以，接下来 Kubernetes 只要获取到这个 PVC 对象，就一定能够找到它所绑定的 PV。

** 所谓容器的 Volume，其实就是将一个宿主机上的目录，跟一个容器里的目录绑定挂载在了一起。**

而所谓的“持久化 Volume”，指的就是这个宿主机上的目录，具备“持久性”。即：这个目录里面的内容，既不会因为容器的删除而被清理掉，也不会跟当前的宿主机绑定。这样，当容器被重启或者在其他节点上重建出来之后，它仍然能够通过挂载这个 Volume，访问到这些内容。

显然，我们前面使用的**  hostPath 和 emptyDir 类型的 Volume ** 并不具备这个特征：它们既有可能被 kubelet 清理掉，也不能被“迁移”到其他节点上。

大多数情况下，持久化 Volume 的实现，往往依赖于一个远程存储服务，比如：远程文件存储（比如，NFS、GlusterFS）、远程块存储（比如，公有云提供的远程磁盘）等等。

**  这个准备“持久化”宿主机目录的过程，我们可以形象地称为“两阶段处理”。** 

1. 当一个 Pod 调度到一个节点上之后，kubelet 就要负责为这个 Pod 创建它的 Volume 目录。默认情况下，kubelet 为 Volume 创建的目录是如下所示的一个宿主机上的路径 ,把这个阶段称为 Attach
2. 磁盘设备格式化并挂载到 Volume 宿主机目录的操作，对应的正是“两阶段处理”的第二个阶段，我们一般称为：Mount。

### StorageClass
大规模集群下手动创建PV几乎不可能.Kubernetes 为我们提供了一套可以自动创建 PV 的机制，即：Dynamic Provisioning。其核心，在于一个名叫 StorageClass 的 API 对象。而 StorageClass 对象的作用，其实就是创建 PV 的模板。
StorageClass 对象会定义如下两个部分内容:

1. PV 的属性。比如，存储类型、Volume 的大小等等。
2. 创建这种 PV 需要用到的存储插件。比如，Ceph 等等。

		apiVersion: storage.k8s.io/v1
		kind: StorageClass
		metadata:
		  name: block-service
		provisioner: kubernetes.io/gce-pd
		parameters:
		  type: pd-ssd
Kubernetes 就能够根据用户提交的 PVC，找到一个对应的 StorageClass 了。然后，Kubernetes 就会调用该 StorageClass 声明的存储插件，创建出需要的 PV。

StorageClass 的 provisioner 字段的值是：kubernetes.io/gce-pd，这正是 Kubernetes 内置的 GCE PD 存储插件的名字。

而这个 StorageClass 的 parameters 字段，就是 PV 的参数。比如：上面例子里的 type=pd-ssd，指的是这个 PV 的类型是“SSD 格式的 GCE 远程磁盘”。

使用ROOK示例:

	apiVersion: ceph.rook.io/v1beta1
	kind: Pool
	metadata:
	  name: replicapool
	  namespace: rook-ceph
	spec:
	  replicated:
	    size: 3
	---
	apiVersion: storage.k8s.io/v1
	kind: StorageClass
	metadata:
	  name: block-service
	provisioner: ceph.rook.io/block
	parameters:
	  pool: replicapool
	  #The value of "clusterNamespace" MUST be the same as the one in which your rook cluster exist
	  clusterNamespace: rook-ceph

应用开发者，我们只需要在 PVC 里指定要使用的 StorageClass 名字即可，如下所示：

	apiVersion: v1
	kind: PersistentVolumeClaim
	metadata:
	  name: claim1
	spec:
	  accessModes:
	    - ReadWriteOnce
	  storageClassName: block-service
	  resources:
	    requests:
	      storage: 30Gi

#### cephs
Rook 项目是一个基于 Ceph 的 Kubernetes 存储插件（它后期也在加入对更多存储实现的支持）。不过，不同于对 Ceph 的简单封装，Rook 在自己的实现中加入了水平扩展、迁移、灾难备份、监控等大量的企业级功能，使得这个项目变成了一个完整的、生产级别可用的容器存储插件。得益于容器化技术，用几条指令，Rook 就可以把复杂的 Ceph 存储后端部署起来：
	
	$ kubectl apply -f https://raw.githubusercontent.com/rook/rook/master/cluster/examples/kubernetes/ceph/crds.yaml
	$ kubectl apply -f https://raw.githubusercontent.com/rook/rook/master/cluster/examples/kubernetes/ceph/common.yaml
	
	$ kubectl apply -f https://raw.githubusercontent.com/rook/rook/master/cluster/examples/kubernetes/ceph/operator.yaml
	
	$ kubectl apply -f https://raw.githubusercontent.com/rook/rook/master/cluster/examples/kubernetes/ceph/cluster.yaml

### “本地”持久化存储 Local Persistent Volume
!! Local Persistent Volume 并不适用于所有应用. 相比于正常的 PV，一旦这些节点宕机且不能恢复时，Local Persistent Volume 的数据就可能丢失。这就要求使用 Local Persistent Volume 的应用必须具备数据备份和恢复的能力，允许你把这些数据定时备份在其他位置。

##### 原则: 绝不应该把一个宿主机上的目录当作 PV 使用
因为，这种本地目录的存储行为完全不可控，它所在的磁盘随时都可能被应用写满，甚至造成整个宿主机宕机。
一个 Local Persistent Volume 对应的存储介质，一定是一块额外挂载在宿主机的磁盘或者块设备

调度器如何保证 Pod 始终能被正确地调度到它所请求的 Local Persistent Volume 所在的节点上?
常规的 PV 来说，Kubernetes 都是先调度 Pod 到某个节点上，然后，再通过“两阶段处理”来“持久化”这台机器上的 Volume 目录，进而完成 Volume 目录与容器的绑定挂载。

可是，对于 Local PV 来说，节点上可供使用的磁盘（或者块设备），必须是运维人员提前准备好的。它们在不同节点上的挂载情况可以完全不同，甚至有的节点可以没这种磁盘。

调度器必须能够知道所有节点与 Local Persistent Volume 对应的磁盘的关联关系，然后根据这个信息来调度 Pod。

##### 实践
在名叫 node-1 的宿主机上创建一个挂载点，比如 /mnt/disks；然后，用几个 RAM Disk 来模拟本地磁盘

	# 在node-1上执行
	$ mkdir /mnt/disks
	$ for vol in vol1 vol2 vol3; do
	    mkdir /mnt/disks/$vol
	    mount -t tmpfs $vol /mnt/disks/$vol
	done

为这些本地磁盘定义对应的 PV:

	apiVersion: v1
	kind: PersistentVolume
	metadata:
	  name: example-pv
	spec:
	  capacity:
	    storage: 5Gi
	  volumeMode: Filesystem
	  accessModes:
	  - ReadWriteOnce
	  persistentVolumeReclaimPolicy: Delete
	  storageClassName: local-storage
	  local:
	    path: /mnt/disks/vol1
	  nodeAffinity:
	    required:
	      nodeSelectorTerms:
	      - matchExpressions:
	        - key: kubernetes.io/hostname
	          operator: In
	          values:
	          - node-1

可以看到，这个 PV 的定义里：local 字段，指定了它是一个 Local Persistent Volume；

而 path 字段，指定的正是这个 PV 对应的本地磁盘的路径，即：/mnt/disks/vol1。

当然了，这也就意味着如果 Pod 要想使用这个 PV，那它就必须运行在 node-1 上。所以，在这个 PV 的定义里，需要有一个 nodeAffinity 字段指定 node-1 这个节点的名字。这样，调度器在调度 Pod 的时候，就能够知道一个 PV 与节点的对应关系，从而做出正确的选择。这正是 Kubernetes 实现“在调度的时候就考虑 Volume 分布”的主要方法。

StorageClass 来描述这个 PV:

	kind: StorageClass
	apiVersion: storage.k8s.io/v1
	metadata:
	  name: local-storage
	provisioner: kubernetes.io/no-provisioner
	volumeBindingMode: WaitForFirstConsumer
这个 StorageClass 的名字，叫作 local-storage。需要注意的是，在它的 provisioner 字段，我们指定的是 no-provisioner。这是因为 Local Persistent Volume 目前尚不支持 Dynamic Provisioning，所以它没办法在用户创建 PVC 的时候，就自动创建出对应的 PV。也就是说，我们前面创建 PV 的操作，是不可以省略的。

与此同时，这个 StorageClass 还定义了一个 volumeBindingMode=WaitForFirstConsumer 的属性。它是 Local Persistent Volume 里一个非常重要的特性，即：延迟绑定。在使用 Local Persistent Volume 的时候，我们必须想办法推迟这个“绑定”操作。否则会出向调度错误问题

定义一个非常普通的 PVC:

	kind: PersistentVolumeClaim
	apiVersion: v1
	metadata:
	  name: example-local-claim
	spec:
	  accessModes:
	  - ReadWriteOnce
	  resources:
	    requests:
	      storage: 5Gi
	  storageClassName: local-storage

编写一个 Pod 来声明使用这个 PVC:

	kind: Pod
	apiVersion: v1
	metadata:
	  name: example-pv-pod
	spec:
	  volumes:
	    - name: example-pv-storage
	      persistentVolumeClaim:
	       claimName: example-local-claim
	  containers:
	    - name: example-pv-container
	      image: nginx
	      ports:
	        - containerPort: 80
	          name: "http-server"
	      volumeMounts:
	        - mountPath: "/usr/share/nginx/html"
	          name: example-pv-storage
需要注意的是，我们上面手动创建 PV 的方式，即 Static 的 PV 管理方式，在删除 PV 时需要按如下流程执行操作：删除使用这个 PV 的 Pod；从宿主机移除本地磁盘（比如，umount 它）；删除 PVC；删除 PV。

## 容器网络部分
![k8s net](k8s/3.jpg)

docker 容器通弄过再宿主机上创建docker0这样一个网桥,他就是一个交换机二层设备,创建的容器通过Veth Pair插入到宿主机的docker0上,实现本机内容器之间的网络互通.对于访问其他主机的情况数据包通过docker0到宿主机的eth0再通过宿主机的路由表或网关发到其他机器上.

### 不同宿主机上`容器之间访问的解决方案?
Flannel 项目是 CoreOS 公司主推的容器网络方案,支持三种后端实现
VXLAN；host-gw；UDP。

UDP模式:
假如container-1(Node1) 容器里的进程发起的 IP 包，其源地址就是 100.96.1.2，目的地址container-1(Node2)就是 100.96.2.3
Flannel 已经在宿主机上创建出了一系列的路由规则，如图

	# 在Node 1上
	$ ip route
	default via 10.168.0.1 dev eth0
	100.96.0.0/16 dev flannel0  proto kernel  scope link  src 100.96.1.0
	100.96.1.0/24 dev docker0  proto kernel  scope link  src 100.96.1.1
	10.168.0.0/24 dev eth0  proto kernel  scope link  src 10.168.0.2
flannel0 设备是一个 TUN 设备（Tunnel 设备）工作在三层（Network Layer）的虚拟网络设备。TUN 设备的功能非常简单，即：在操作系统内核和用户应用程序之间传递 IP 包。

flannel0 就会把这个 IP 包，交给创建这个设备的应用程序，也就是 Flannel 进程。Flannel 进程 看到了这个 IP 包的目的地址，是 100.96.2.3，flanneld 在收到 container-1 发给 container-2 的 IP 包之后，就会把这个 IP 包直接封装在一个 UDP 包里，然后发送给 Node 2。

flanneld 又是如何知道这个 IP 地址对应的容器，是运行在 Node 2 上的呢？

事实上，在由 Flannel 管理的容器网络里，一台宿主机上的所有容器，都属于该宿主机被分配的一个“子网”。在我们的例子中，Node 1 的子网是 100.96.1.0/24，container-1 的 IP 地址是 100.96.1.2。Node 2 的子网是 100.96.2.0/24，container-2 的 IP 地址是 100.96.2.3。而这些子网与宿主机的对应关系，正是保存在 Etcd 当中

每台宿主机上的 flanneld，都监听着一个 8285 端口，所以 flanneld 只要把 UDP 包发往 Node 2 的 8285 端口即可。Node 2 上监听 8285 端口的进程也是 flanneld，所以这时候，flanneld 就可以从这个 UDP 包里解析出封装在里面的、container-1 发来的原 IP 包,直接把这个 IP 包发送给它所管理的 TUN 设备，即 flannel0 设备。这正是一个从用户态向内核态的流动方向.Node 2 上的路由表，跟 Node 1 非常类似,Linux 内核就会按照这条路由规则，把这个 IP 包转发给 docker0 网桥。
![UDP](k8s/4.jpg)

缺点:一次网络请求经过了多次用户态内核态的切换复制,性能低下!!!

VXLAN模式:即 Virtual Extensible LAN（虚拟可扩展局域网）

VXLAN 的覆盖网络的设计思想是：在现有的三层网络之上，“覆盖”一层虚拟的、由内核 VXLAN 模块负责维护的二层网络，使得连接在这个 VXLAN 二层网络上的“主机”（虚拟机或者容器都可以）之间，可以像在同一个局域网（LAN）里那样自由通信。当然，实际上，这些“主机”可能分布在不同的宿主机上，甚至是分布在不同的物理机房里。

而为了能够在二层网络上打通“隧道”，VXLAN 会在宿主机上设置一个特殊的网络设备作为“隧道”的两端。这个设备就叫作 VTEP，即：VXLAN Tunnel End Point（虚拟隧道端点）。而 VTEP 设备的作用，其实跟前面的 flanneld 进程非常相似。只不过，它进行封装和解封装的对象，是二层数据帧（Ethernet frame）；而且这个工作的执行流程，全部是在内核里完成的（因为 VXLAN 本身就是 Linux 内核中的一个模块）。


### k8s网络
Kubernetes 是通过一个叫作 CNI 的接口，维护了一个单独的网桥来代替 docker0。这个网桥的名字就叫作：CNI 网桥，它在宿主机上的设备名称默认是：cni0, k8s通过网桥类型的 Flannel 插件来实现UDP和VXLAN模式

host-gw模式:一种纯三层（Pure Layer 3）网络方案,Flannel 的 host-gw 模式和 Calico 项目
![host-gw](k8s/5.jpg)
假设现在，Node 1 上的 Infra-container-1，要访问 Node 2 上的 Infra-container-2。当你设置 Flannel 使用 host-gw 模式之后，flanneld 会在宿主机上创建这样一条规则，以 Node 1 为例：

	$ ip route
	...
	10.244.1.0/24 via 10.168.0.3 dev eth0

这条路由规则的含义是：目的 IP 地址属于 10.244.1.0/24 网段的 IP 包，应该经过本机的 eth0 设备发出去（即：dev eth0）；并且，它下一跳地址（next-hop）是 10.168.0.3（即：via 10.168.0.3）。

这个下一跳地址对应的，正是我们的目的宿主机 Node 2。

当 IP 包从网络层进入链路层封装成帧的时候，eth0 设备就会使用下一跳地址对应的 MAC 地址，作为该数据帧的目的 MAC 地址。显然，这个 MAC 地址，正是 Node 2 的 MAC 地址。这样，这个数据帧就会从 Node 1 通过宿主机的二层网络顺利到达 Node 2 上。

而 Node 2 的内核网络栈从二层数据帧里拿到 IP 包后，会“看到”这个 IP 包的目的 IP 地址是 10.244.1.3，即 Infra-container-2 的 IP 地址。这时候，根据 Node 2 上的路由表，该目的地址会匹配到第二条路由规则（也就是 10.244.1.0 对应的路由规则），从而进入 cni0 网桥，进而进入到 Infra-container-2 当中。

当然，Flannel 子网和主机的信息，都是保存在 Etcd 当中的。flanneld 只需要 WACTH 这些数据的变化，然后实时更新路由表即可。

#### Calico 项目
![Calico](k8s/6.jpg)

### K8s网络隔离
NetworkPolicy

	apiVersion: networking.k8s.io/v1
	kind: NetworkPolicy
	metadata:
	  name: test-network-policy
	  namespace: default
	spec:
	  podSelector:
	    matchLabels:
	      role: db
	  policyTypes:
	  - Ingress
	  - Egress
	  ingress:
	  - from:
	    - ipBlock:
	        cidr: 172.17.0.0/16
	        except:
	        - 172.17.1.0/24
	    - namespaceSelector:
	        matchLabels:
	          project: myproject
	    - podSelector:
	        matchLabels:
	          role: frontend
	    ports:
	    - protocol: TCP
	      port: 6379
	  egress:
	  - to:
	    - ipBlock:
	        cidr: 10.0.0.0/24
	    ports:
	    - protocol: TCP
	      port: 5978
podSelector 字段。它的作用，就是定义这个 NetworkPolicy 的限制范围，比如：当前 Namespace 里携带了 role=db 标签的 Pod。而如果你把 podSelector 字段留空：

	spec:
	 podSelector: {}
那么这个 NetworkPolicy 就会作用于当前 Namespace 下的所有 Pod。而一旦 Pod 被 NetworkPolicy 选中，那么这个 Pod 就会进入“拒绝所有”（Deny All）的状态，即：这个 Pod 既不允许被外界访问，也不允许对外界发起访问。而 NetworkPolicy 定义的规则，其实就是“白名单”。

例如，在我们上面这个例子里，我在 policyTypes 字段，定义了这个 NetworkPolicy 的类型是 ingress 和 egress，即：它既会影响流入（ingress）请求，也会影响流出（egress）请求。

然后，在 ingress 字段里，我定义了 from 和 ports，即：允许流入的“白名单”和端口。其中，这个允许流入的“白名单”里，我指定了三种并列的情况，分别是：ipBlock、namespaceSelector 和 podSelector。

而在 egress 字段里，我则定义了 to 和 ports，即：允许流出的“白名单”和端口。这里允许流出的“白名单”的定义方法与 ingress 类似。只不过，这一次 ipblock 字段指定的，是目的地址的网段。

综上所述，这个 NetworkPolicy 对象，指定的隔离规则如下所示
1. 该隔离规则只对 default Namespace 下的，携带了 role=db 标签的 Pod 有效。限制的请求类型包括 ingress（流入）和 egress（流出）。
2. Kubernetes 会拒绝任何访问被隔离 Pod 的请求，除非这个请求来自于以下“白名单”里的对象，并且访问的是被隔离 Pod 的 6379 端口。这些“白名单”对象包括：a. default Namespace 里的，携带了 role=fronted 标签的 Pod；b. 携带了 project=myproject 标签的 Namespace 里的任何 Pod；c. 任何源地址属于 172.17.0.0/16 网段，且不属于 172.17.1.0/24 网段的请求。
3. Kubernetes 会拒绝被隔离 Pod 对外发起任何请求，除非请求的目的地址属于 10.0.0.0/24 网段，并且访问的是该网段地址的 5978 端口。

需要注意的是，定义一个 NetworkPolicy 对象的过程，容易犯错的是“白名单”部分（from 和 to 字段）。举个例子：

	  ...
	  ingress:
	  - from:
	    - namespaceSelector:
	        matchLabels:
	          user: alice
	    - podSelector:
	        matchLabels:
	          role: client
	  ...

像上面这样定义的 namespaceSelector 和 podSelector，是“或”（OR）的关系。所以说，这个 from 字段定义了两种情况，无论是 Namespace 满足条件，还是 Pod 满足条件，这个 NetworkPolicy 都会生效。

下面这个例子，虽然看起来类似，但是它定义的规则却完全不同：

	...
	  ingress:
	  - from:
	    - namespaceSelector:
	        matchLabels:
	          user: alice
	      podSelector:
	        matchLabels:
	          role: client
	  ...
注意看，这样定义的 namespaceSelector 和 podSelector，其实是“与”（AND）的关系。所以说，这个 from 字段只定义了一种情况，只有 Namespace 和 Pod 同时满足条件，这个 NetworkPolicy 才会生效。

此外，如果要使上面定义的 NetworkPolicy 在 Kubernetes 集群里真正产生作用，你的 CNI 网络插件就必须是支持 Kubernetes 的 NetworkPolicy 的。
在 Kubernetes 生态里，目前已经实现了 NetworkPolicy 的网络插件包括 Calico、Weave 和 kube-router 等多个项目，但是并不包括 Flannel 项目。

示例:

	apiVersion: extensions/v1beta1
	kind: NetworkPolicy
	metadata:
	  name: test-network-policy
	  namespace: default
	spec:
	  podSelector:
	    matchLabels:
	      role: db
	  ingress:
	   - from:
	     - namespaceSelector:
	         matchLabels:
	           project: myproject
	     - podSelector:
	         matchLabels:
	           role: frontend
	     ports:
	       - protocol: tcp
	         port: 6379
可以看到，我们指定的 ingress“白名单”，是任何 Namespace 里，携带 project=myproject 标签的 Namespace 里的 Pod；以及 default Namespace 里，携带了 role=frontend 标签的 Pod。允许被访问的端口是：6379。而被隔离的对象，是所有携带了 role=db 标签的 Pod。

### Service、DNS 与服务发现

Kubernetes 之所以需要 Service，一方面是因为 Pod 的 IP 不是固定的，另一方面则是因为一组 Pod 实例之间总会有负载均衡的需求。
Service的配置文件:

	apiVersion: v1
	kind: Service
	metadata:
	  name: bsp-service
	  labels:
	    app: bsp-service
	spec:
	  ports:
	  - port: 80
	    name: bsp-web
	    targetPort: 80
	    protocol: TCP
	  selector:
	    app: bsp
selector 字段来声明这个 Service 只代理携带了 app=bsp 标签的 Pod。并且，这个 Service 的 80 端口，代理的是 Pod 的 80 端口。
Deployment文件:

	apiVersion: apps/v1
	kind: Deployment
	metadata:
	  name: bsp-deployment
	spec:
	  selector:
	    matchLabels:
	      app: bsp
	  replicas: 3
	  template:
	    metadata:
	      labels:
	        app: bsp
	    spec:
	      containers:
	      - name: bsp
	        image: 192.168.111.1:5000/bsp:1.0
	        env:
	        - name: SERVER_PORT
	          value: "80"
	        ports:
	        - containerPort: 80
	        volumeMounts:
	        - mountPath: "/root/logs/bsp/"
	          name: bsp-log
	      volumes:
	      - name: bsp-log
	        hostPath:
	          path: "/root/logs/bsp/"
selector 选中的 Pod，就称为 Service 的 Endpoints，你可以使用 kubectl get ep 命令看到它们，如下所示：

	[root@k8s-master-node1 ~]# kubectl apply -f bsp.yml 
	deployment.apps/bsp-deployment created
	[root@k8s-master-node1 ~]# kubectl apply -f bsp-service.yml 
	service/bsp-service created
	[root@k8s-master-node1 ~]# kubectl get service
	NAME               TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE
	bsp-service        ClusterIP   10.96.195.78    <none>        80/TCP    11s
	ingress-demo-app   ClusterIP   10.96.114.172   <none>        80/TCP    5d20h
	kubernetes         ClusterIP   10.96.0.1       <none>        443/TCP   5d20h
	[root@k8s-master-node1 ~]# kubectl de service
	debug     delete    describe  
	[root@k8s-master-node1 ~]# kubectl describe service bsp-service 
	Name:              bsp-service
	Namespace:         default
	Labels:            app=bsp-service
	Annotations:       <none>
	Selector:          app=bsp
	Type:              ClusterIP
	IP Family Policy:  SingleStack
	IP Families:       IPv4
	IP:                10.96.195.78
	IPs:               10.96.195.78
	Port:              bsp-web  80/TCP
	TargetPort:        80/TCP
	Endpoints:         10.244.0.5:80,10.244.0.6:80,10.244.0.7:80
	Session Affinity:  None
	Events:            <none>
	$ kubectl get endpoints bsp-service
	NAME        ENDPOINTS
	bsp-service   10.244.0.5:80,10.244.0.6:80,10.244.0.7:80

只有处于 Running 状态，且 readinessProbe 检查通过的 Pod，才会出现在 Service 的 Endpoints 列表里。并且，当某一个 Pod 出现问题时，Kubernetes 会自动把它从 Service 里摘除掉。

该 Service 的 VIP 地址 10.96.195.78，在kubectl get service的clusterip那一栏,你就可以访问到它所代理的 Pod 了

	[root@k8s-master-node1 ~]# curl 10.96.195.78:80/user/login
	{"success":false,"msg":null,"object":null}
	[root@k8s-master-node1 ~]#
	$ curl 10.0.1.175:80
	hostnames-0uton
	$ curl 10.0.1.175:80
	hostnames-yp2kp
	$ curl 10.0.1.175:80
	hostnames-bvc05
Service 提供的是 Round Robin 方式的负载均衡。对于这种方式，我们称为：ClusterIP 模式的 Service
原理:
kube-proxy 组件，加上 iptables 来共同实现的。10.96.195.78 只是一条 iptables 规则上的配置，并没有真正的网络设备，所以你 ping 这个地址，是不会有任何响应的.每个Service会在每个机器上维护一个虚拟IP的iptables 规则链,去访问其下的所有POD,你的宿主机上有大量 Pod 的时候，成百上千条 iptables 规则不断地被刷新，会大量占用该宿主机的 CPU 资源.

解决方案: IPVS 模式的 Service
IPVS 模式kube-proxy 首先会在宿主机上创建一个虚拟网卡（叫作：kube-ipvs0），并为它分配 Service VIP 作为 IP 地址

	# ip addr
	  ...
	  73：kube-ipvs0：<BROADCAST,NOARP>  mtu 1500 qdisc noop state DOWN qlen 1000
	  link/ether  1a:ce:f5:5f:c1:4d brd ff:ff:ff:ff:ff:ff
	  inet 10.0.1.175/32  scope global kube-ipvs0
	  valid_lft forever  preferred_lft forever

kube-proxy 就会通过 Linux 的 IPVS 模块，为这个 IP 地址设置三个 IPVS 虚拟主机，并设置这三个虚拟主机之间使用轮询模式 (rr) 来作为负载均衡策略。我们可以通过 ipvsadm 查看到这个设置
	
	# ipvsadm -ln
	 IP Virtual Server version 1.2.1 (size=4096)
	  Prot LocalAddress:Port Scheduler Flags
	    ->  RemoteAddress:Port           Forward  Weight ActiveConn InActConn     
	  TCP  10.102.128.4:80 rr
	    ->  10.244.3.6:9376    Masq    1       0          0         
	    ->  10.244.1.7:9376    Masq    1       0          0
	    ->  10.244.2.3:9376    Masq    1       0          0
在大规模集群里，我非常建议你为 kube-proxy 设置–proxy-mode=ipvs 来开启这个功能。它为 Kubernetes 集群规模带来的提升，还是非常巨大的。

### 从外部访问Servicve
最常用的一种方式就是：NodePort

	apiVersion: v1
	kind: Service
	metadata:
	  name: bsp-service
	  labels:
	    run: bsp-service
	spec:
	  type: NodePort
	  ports:
	  - nodePort: 80
	    targetPort: 80
	    protocol: TCP
	    name: http
	  - nodePort: 443
	    protocol: TCP
	    name: https
	  selector:
	    app: bsp
在这个 Service 的定义里，我们声明它的类型是，type=NodePort。然后，我在 ports 字段里声明了 Service 的 80 端口代理 Pod 的 80 端口，Service 的 443 端口代理 Pod 的 443 端口。
默认指定端口有限制30000-50000
vim /etc/kubernetes/manifests/kube-apiserver.yaml
找到 --service-cluster-ip-range修改为自己想要的范围

	systemctl daemon-reload
	systemctl restart kubelet

要访问这个 Service，你只需要访问：<任何一台宿主机的IP地址>:80

第二种访问方式:LoadBalancer

	---
	kind: Service
	apiVersion: v1
	metadata:
	  name: example-service
	spec:
	  ports:
	  - port: 8765
	    targetPort: 9376
	  selector:
	    app: example
	  type: LoadBalancer

适用于公有云上的 Kubernetes 服务

在公有云提供的 Kubernetes 服务里，都使用了一个叫作 CloudProvider 的转接层，来跟公有云本身的 API 进行对接。所以，在上述 LoadBalancer 类型的 Service 被提交后，Kubernetes 就会调用 CloudProvider 在公有云上为你创建一个负载均衡服务，并且把被代理的 Pod 的 IP 地址配置给负载均衡服务做后端。

第三种方式: ExternalName

	kind: Service
	apiVersion: v1
	metadata:
	  name: my-service
	spec:
	  type: ExternalName
	  externalName: my.database.example.com
指定了一个 externalName=my.database.example.com 的字段。而且你应该会注意到，这个 YAML 文件里不需要指定 selector。

这时候，当你通过 Service 的 DNS 名字访问它的时候，比如访问：my-service.default.svc.cluster.local。那么，Kubernetes 为你返回的就是my.database.example.com。所以说，ExternalName 类型的 Service，其实是在 kube-dns 里为你添加了一条 CNAME 记录。这时，访问 my-service.default.svc.cluster.local 就和访问 my.database.example.com 这个域名是一个效果了。

Kubernetes 的 Service 还允许你为 Service 分配公有 IP 地址

	kind: Service
	apiVersion: v1
	metadata:
	  name: my-service
	spec:
	  selector:
	    app: MyApp
	  ports:
	  - name: http
	    protocol: TCP
	    port: 80
	    targetPort: 9376
	  externalIPs:
	  - 80.11.12.10
externalIPs=80.11.12.10，那么此时，你就可以通过访问 80.11.12.10:80 访问到被代理的 Pod 了。不过，在这里 Kubernetes 要求 externalIPs 必须是至少能够路由到一个 Kubernetes 的节点.

常见问题排查思路:
Service 没办法通过 DNS 访问到的时候。你就需要区分到底是 Service 本身的配置问题，还是集群的 DNS 出了问题。一个行之有效的方法，就是检查 Kubernetes 自己的 Master 节点的 Service DNS 是否正常：

	# 在一个Pod里执行
	$ nslookup kubernetes.default
	Server:    10.0.0.10
	Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local
	
	Name:      kubernetes.default
	Address 1: 10.0.0.1 kubernetes.default.svc.cluster.local
如果上面访问 kubernetes.default 返回的值都有问题，那你就需要检查 kube-dns 的运行状态和日志了。否则的话，你应该去检查自己的 Service 定义是不是有问题。

而如果你的 Service 没办法通过 ClusterIP 访问到的时候，你首先应该检查的是这个 Service 是否有 Endpoints：

	$ kubectl get endpoints hostnames
	NAME        ENDPOINTS
	hostnames   10.244.0.5:9376,10.244.0.6:9376,10.244.0.7:9376
需要注意的是，如果你的 Pod 的 readniessProbe 没通过，它也不会出现在 Endpoints 列表里。而如果 Endpoints 正常，那么你就需要确认 kube-proxy 是否在正确运行。在我们通过 kubeadm 部署的集群里，你应该看到 kube-proxy 输出的日志如下所示：

	I1027 22:14:53.995134    5063 server.go:200] Running in resource-only container "/kube-proxy"
	I1027 22:14:53.998163    5063 server.go:247] Using iptables Proxier.
	I1027 22:14:53.999055    5063 server.go:255] Tearing down userspace rules. Errors here are acceptable.
	I1027 22:14:54.038140    5063 proxier.go:352] Setting endpoints for "kube-system/kube-dns:dns-tcp" to [10.244.1.3:53]
	I1027 22:14:54.038164    5063 proxier.go:352] Setting endpoints for "kube-system/kube-dns:dns" to [10.244.1.3:53]
	I1027 22:14:54.038209    5063 proxier.go:352] Setting endpoints for "default/kubernetes:https" to [10.240.0.2:443]
	I1027 22:14:54.038238    5063 proxier.go:429] Not syncing iptables until Services and Endpoints have been received from master
	I1027 22:14:54.040048    5063 proxier.go:294] Adding new service "default/kubernetes:https" at 10.0.0.1:443/TCP
	I1027 22:14:54.040154    5063 proxier.go:294] Adding new service "kube-system/kube-dns:dns" at 10.0.0.10:53/UDP
	I1027 22:14:54.040223    5063 proxier.go:294] Adding new service "kube-system/kube-dns:dns-tcp" at 10.0.0.10:53/TCP
如果 kube-proxy 一切正常，你就应该仔细查看宿主机上的 iptables 了。而一个 iptables 模式的 Service 对应的规则，我在上一篇以及这一篇文章里已经全部介绍到了，它们包括：
1. KUBE-SERVICES 或者 KUBE-NODEPORTS 规则对应的 Service 的入口链，这个规则应该与 VIP 和 Service 端口一一对应；
2. KUBE-SEP-(hash) 规则对应的 DNAT 链，这些规则应该与 Endpoints 一一对应；
3. KUBE-SVC-(hash) 规则对应的负载均衡链，这些规则的数目应该与 Endpoints 数目一致；
4. 如果是 NodePort 模式的话，还有 POSTROUTING 处的 SNAT 链。

通过查看这些链的数量、转发目的地址、端口、过滤条件等信息，你就能很容易发现一些异常的蛛丝马迹。

还有一种典型问题，就是 Pod 没办法通过 Service 访问到自己。这往往就是因为 kubelet 的 hairpin-mode 没有被正确设置。关于 Hairpin 的原理我在前面已经介绍过，这里就不再赘述了。你只需要确保将 kubelet 的 hairpin-mode 设置为 hairpin-veth 或者 promiscuous-bridge 即可。

其中，在 hairpin-veth 模式下，你应该能看到 CNI 网桥对应的各个 VETH 设备，都将 Hairpin 模式设置为了 1，如下所示：

	$ for d in /sys/devices/virtual/net/cni0/brif/veth*/hairpin_mode; do echo "$d = $(cat $d)"; done
	/sys/devices/virtual/net/cni0/brif/veth4bfbfe74/hairpin_mode = 1
	/sys/devices/virtual/net/cni0/brif/vethfc2a18c5/hairpin_mode = 1

而如果是 promiscuous-bridge 模式的话，你应该看到 CNI 网桥的混杂模式（PROMISC）被开启，如下所示：

	$ ifconfig cni0 |grep PROMISC
	UP BROADCAST RUNNING PROMISC MULTICAST  MTU:1460  Metric:1
*** 
### Service与Ingress
Service 都要有一个负载均衡服务，所以这个做法实际上既浪费成本又高。作为用户，我其实更希望看到 Kubernetes 为我内置一个全局的负载均衡器。然后，通过我访问的 URL，把请求转发给不同的后端 Service。

这种全局的、为了代理不同后端 Service 而设置的负载均衡服务，就是 Kubernetes 里的 Ingress 服务。

假如我现在有这样一个站点：https://cafe.example.com。其中，https://cafe.example.com/coffee，对应的是“咖啡点餐系统”。而，https://cafe.example.com/tea，对应的则是“茶水点餐系统”。这两个系统，分别由名叫 coffee 和 tea 这样两个 Deployment 来提供服务。

那么现在，我如何能使用 Kubernetes 的 Ingress 来创建一个统一的负载均衡器，从而实现当用户访问不同的域名时，能够访问到不同的 Deployment 呢？

在 Kubernetes 里就需要通过 Ingress 对象来描述，如下所示：

	apiVersion: extensions/v1beta1
	kind: Ingress
	metadata:
	  name: cafe-ingress
	spec:
	  tls:
	  - hosts:
	    - cafe.example.com
	    secretName: cafe-secret
	  rules:
	  - host: cafe.example.com
	    http:
	      paths:
	      - path: /tea
	        backend:
	          serviceName: tea-svc
	          servicePort: 80
	      - path: /coffee
	        backend:
	          serviceName: coffee-svc
	          servicePort: 80
在上面这个名叫 cafe-ingress.yaml 文件中，最值得我们关注的，是 rules 字段。在 Kubernetes 里，这个字段叫作：IngressRule。

IngressRule 的 Key，就叫做：host。它必须是一个标准的域名格式（Fully Qualified Domain Name）的字符串，而不能是 IP 地址。

而 host 字段定义的值，就是这个 Ingress 的入口。这也就意味着，当用户访问 cafe.example.com 的时候，实际上访问到的是这个 Ingress 对象。这样，Kubernetes 就能使用 IngressRule 来对你的请求进行下一步转发。

而接下来 IngressRule 规则的定义，则依赖于 path 字段。你可以简单地理解为，这里的每一个 path 都对应一个后端 Service。所以在我们的例子里，我定义了两个 path，它们分别对应 coffee 和 tea 这两个 Deployment 的 Service（即：coffee-svc 和 tea-svc）。

一个 Ingress 对象的主要内容，实际上就是一个“反向代理”服务（比如：Nginx）的配置文件的描述。而这个代理服务对应的转发规则，就是 IngressRule。

这就是为什么在每条 IngressRule 里，需要有一个 host 字段来作为这条 IngressRule 的入口，然后还需要有一系列 path 字段来声明具体的转发策略。这其实跟 Nginx、HAproxy 等项目的配置文件的写法是一致的。

然后，这个 Ingress Controller 会根据你定义的 Ingress 对象，提供对应的代理能力。目前，业界常用的各种反向代理项目，比如 Nginx、HAProxy、Envoy、Traefik 等，都已经为 Kubernetes 专门维护了对应的 Ingress Controller。

接下来，我就以最常用的 Nginx Ingress Controller 为例，在我们前面用 kubeadm 部署的 Bare-metal 环境中，和你实践一下 Ingress 机制的使用过程。

	$ kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/mandatory.yaml

	kind: ConfigMap
	apiVersion: v1
	metadata:
	  name: nginx-configuration
	  namespace: ingress-nginx
	  labels:
	    app.kubernetes.io/name: ingress-nginx
	    app.kubernetes.io/part-of: ingress-nginx
	---
	apiVersion: extensions/v1beta1
	kind: Deployment
	metadata:
	  name: nginx-ingress-controller
	  namespace: ingress-nginx
	  labels:
	    app.kubernetes.io/name: ingress-nginx
	    app.kubernetes.io/part-of: ingress-nginx
	spec:
	  replicas: 1
	  selector:
	    matchLabels:
	      app.kubernetes.io/name: ingress-nginx
	      app.kubernetes.io/part-of: ingress-nginx
	  template:
	    metadata:
	      labels:
	        app.kubernetes.io/name: ingress-nginx
	        app.kubernetes.io/part-of: ingress-nginx
	      annotations:
	        ...
	    spec:
	      serviceAccountName: nginx-ingress-serviceaccount
	      containers:
	        - name: nginx-ingress-controller
	          image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.20.0
	          args:
	            - /nginx-ingress-controller
	            - --configmap=$(POD_NAMESPACE)/nginx-configuration
	            - --publish-service=$(POD_NAMESPACE)/ingress-nginx
	            - --annotations-prefix=nginx.ingress.kubernetes.io
	          securityContext:
	            capabilities:
	              drop:
	                - ALL
	              add:
	                - NET_BIND_SERVICE
	            # www-data -> 33
	            runAsUser: 33
	          env:
	            - name: POD_NAME
	              valueFrom:
	                fieldRef:
	                  fieldPath: metadata.name
	            - name: POD_NAMESPACE
	            - name: http
	              valueFrom:
	                fieldRef:
	                  fieldPath: metadata.namespace
	          ports:
	            - name: http
	              containerPort: 80
	            - name: https
	              containerPort: 443
上述 YAML 文件中，我们定义了一个使用 nginx-ingress-controller 镜像的 Pod。需要注意的是，这个 Pod 的启动命令需要使用该 Pod 所在的 Namespace 作为参数。而这个信息，当然是通过 Downward API 拿到的，即：Pod 的 env 字段里的定义（env.valueFrom.fieldRef.fieldPath）。

而这个 Pod 本身，就是一个监听 Ingress 对象以及它所代理的后端 Service 变化的控制器。

当一个新的 Ingress 对象由用户创建后，nginx-ingress-controller 就会根据 Ingress 对象里定义的内容，生成一份对应的 Nginx 配置文件（/etc/nginx/nginx.conf），并使用这个配置文件启动一个 Nginx 服务。

而一旦 Ingress 对象被更新，nginx-ingress-controller 就会更新这个配置文件。需要注意的是，如果这里只是被代理的 Service 对象被更新，nginx-ingress-controller 所管理的 Nginx 服务是不需要重新加载（reload）的。这当然是因为 nginx-ingress-controller 通过Nginx Lua方案实现了 Nginx Upstream 的动态配置。

此外，nginx-ingress-controller 还允许你通过 Kubernetes 的 ConfigMap 对象来对上述 Nginx 配置文件进行定制。这个 ConfigMap 的名字，需要以参数的方式传递给 nginx-ingress-controller。而你在这个 ConfigMap 里添加的字段，将会被合并到最后生成的 Nginx 配置文件当中。

可以看到，一个 Nginx Ingress Controller 为你提供的服务，其实是一个可以根据 Ingress 对象和被代理后端 Service 的变化，来自动进行更新的 Nginx 负载均衡器。

为了让用户能够用到这个 Nginx，我们就需要创建一个 Service 来把 Nginx Ingress Controller 管理的 Nginx 服务暴露出去，如下所示：

	$ kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/provider/baremetal/service-nodeport.yaml
由于我们使用的是 Bare-metal 环境，所以 service-nodeport.yaml 文件里的内容，就是一个 NodePort 类型的 Service，如下所示：

	apiVersion: v1
	kind: Service
	metadata:
	  name: ingress-nginx
	  namespace: ingress-nginx
	  labels:
	    app.kubernetes.io/name: ingress-nginx
	    app.kubernetes.io/part-of: ingress-nginx
	spec:
	  type: NodePort
	  ports:
	    - name: http
	      port: 80
	      targetPort: 80
	      protocol: TCP
	    - name: https
	      port: 443
	      targetPort: 443
	      protocol: TCP
	  selector:
	    app.kubernetes.io/name: ingress-nginx
	    app.kubernetes.io/part-of: ingress-nginx
可以看到，这个 Service 的唯一工作，就是将所有携带 ingress-nginx 标签的 Pod 的 80 和 433 端口暴露出去。而如果你是公有云上的环境，你需要创建的就是 LoadBalancer 类型的 Service 了。

首先，我们要在集群里部署我们的应用 Pod 和它们对应的 Service

	$ kubectl create -f cafe.yaml

然后，我们需要创建 Ingress 所需的 SSL 证书（tls.crt）和密钥（tls.key），这些信息都是通过 Secret 对象定义好的，如下所示：

	$ kubectl create -f cafe-secret.yaml

这一步完成后，我们就可以创建在本篇文章一开始定义的 Ingress 对象了，如下所示：

	$ kubectl create -f cafe-ingress.yaml
	查看一下这个 Ingress 对象的信息，如下所示：
	$ kubectl get ingress
	NAME           HOSTS              ADDRESS   PORTS     AGE
	cafe-ingress   cafe.example.com             80, 443   2h
	
	$ kubectl describe ingress cafe-ingress
	Name:             cafe-ingress
	Namespace:        default
	Address:          
	Default backend:  default-http-backend:80 (<none>)
	TLS:
	  cafe-secret terminates cafe.example.com
	Rules:
	  Host              Path  Backends
	  ----              ----  --------
	  cafe.example.com  
	                    /tea      tea-svc:80 (<none>)
	                    /coffee   coffee-svc:80 (<none>)
	Annotations:
	Events:
	  Type    Reason  Age   From                      Message
	  ----    ------  ----  ----                      -------
	  Normal  CREATE  4m    nginx-ingress-controller  Ingress default/cafe-ingress

当然，在 Ingress 的 YAML 文件里，你还可以定义多个 Host，比如restaurant.example.com、movie.example.com等等，来为更多的域名提供负载均衡服务。

如果我的请求没有匹配到任何一条 IngressRule，那么会发生什么呢？首先，既然 Nginx Ingress Controller 是用 Nginx 实现的，那么它当然会为你返回一个 Nginx 的 404 页面。


***
# Kubernetes的资源模型与资源管理

	apiVersion: v1
	kind: Pod
	metadata:
	  name: frontend
	spec:
	  containers:
	  - name: db
	    image: mysql
	    env:
	    - name: MYSQL_ROOT_PASSWORD
	      value: "password"
	    resources:
	      requests:
	        memory: "64Mi"
	        cpu: "250m"
	      limits:
	        memory: "128Mi"
	        cpu: "500m"
	  - name: wp
	    image: wordpress
	    resources:
	      requests:
	        memory: "64Mi"
	        cpu: "250m"
	      limits:
	        memory: "128Mi"
	        cpu: "500m"
当 Pod 里的每一个 Container 都同时设置了 requests 和 limits，并且 requests 和 limits 值相等的时候，这个 Pod 就属于 Guaranteed 类别

在使用容器的时候，你可以通过设置 cpuset 把容器绑定到某个 CPU 的核上，而不是像 cpushare 那样共享 CPU 的计算能力。

这种情况下，由于操作系统在 CPU 之间进行上下文切换的次数大大减少，容器里应用的性能会得到大幅提升。事实上，cpuset 方式，是生产环境里部署在线应用类型的 Pod 时，非常常用的一种方式。

需求在 Kubernetes 里又该如何实现呢？

1. 首先，你的 Pod 必须是 Guaranteed 的 QoS 类型
2. 然后，你只需要将 Pod 的 CPU 资源的 requests 和 limits 设置为同一个相等的整数值即可。


	spec:
	  containers:
	  - name: nginx
	    image: nginx
	    resources:
	      limits:
	        memory: "200Mi"
	        cpu: "2"
	      requests:
	        memory: "200Mi"
	        cpu: "2"
这时候，该 Pod 就会被绑定在 2 个独占的 CPU 核上。当然，具体是哪两个 CPU 核，是由 kubelet 为你分配的。

### pod优先级调度
Kubernetes 里，优先级和抢占机制是在 1.10 版本后才逐步可用的。要使用这个机制，你首先需要在 Kubernetes 里提交一个 PriorityClass 的定义，如下所示：

	apiVersion: scheduling.k8s.io/v1beta1
	kind: PriorityClass
	metadata:
	  name: high-priority
	value: 1000000
	globalDefault: false
	description: "This priority class should be used for high priority service pods only."
上面这个 YAML 文件，定义的是一个名叫 high-priority 的 PriorityClass，其中 value 的值是 1000000 （一百万）。

Kubernetes 规定，优先级是一个 32 bit 的整数，最大值不超过 1000000000（10 亿，1 billion），并且值越大代表优先级越高。而超出 10 亿的值，其实是被 Kubernetes 保留下来分配给系统 Pod 使用的。显然，这样做的目的，就是保证系统 Pod 不会被用户抢占掉。

而一旦上述 YAML 文件里的 globalDefault 被设置为 true 的话，那就意味着这个 PriorityClass 的值会成为系统的默认值。而如果这个值是 false，就表示我们只希望声明使用该 PriorityClass 的 Pod 拥有值为 1000000 的优先级，而对于没有声明 PriorityClass 的 Pod 来说，它们的优先级就是 0。

在创建了 PriorityClass 对象之后，Pod 就可以声明使用它了，如下所示：

	apiVersion: v1
	kind: Pod
	metadata:
	  name: nginx
	  labels:
	    env: test
	spec:
	  containers:
	  - name: nginx
	    image: nginx
	    imagePullPolicy: IfNotPresent
	  priorityClassName: high-priority

## Prometheus、Metrics Server与Kubernetes监控体系
![Prometheus](k8s/7.jpg)

Prometheus 项目工作的核心，是使用 Pull （抓取）的方式去搜集被监控对象的 Metrics 数据（监控指标数据），然后，再把这些数据保存在一个 TSDB （时间序列数据库，比如 OpenTSDB、InfluxDB 等）当中，以便后续可以按照时间进行检索。

第一种 Metrics，是宿主机的监控数据。这部分数据的提供，需要借助一个由 Prometheus 维护的Node Exporter 工具。一般来说，Node Exporter 会以 DaemonSet 的方式运行在宿主机上。其实，所谓的 Exporter，就是代替被监控对象来对 Prometheus 暴露出可以被“抓取”的 Metrics 信息的一个辅助进程。
而 Node Exporter 可以暴露给 Prometheus 采集的 Metrics 数据， 也不单单是节点的负载（Load）、CPU 、内存、磁盘以及网络这样的常规信息，它的 Metrics 指标可以说是“包罗万象”。

第二种 Metrics，是来自于 Kubernetes 的 API Server、kubelet 等组件的 /metrics API。除了常规的 CPU、内存的信息外，这部分信息还主要包括了各个组件的核心监控指标。比如，对于 API Server 来说，它就会在 /metrics API 里，暴露出各个 Controller 的工作队列（Work Queue）的长度、请求的 QPS 和延迟数据等等。这些信息，是检查 Kubernetes 本身工作情况的主要依据。

第三种 Metrics，是 Kubernetes 相关的监控数据。这部分数据，一般叫作 Kubernetes 核心监控数据（core metrics）。这其中包括了 Pod、Node、容器、Service 等主要 Kubernetes 核心概念的 Metrics。

需要注意的是，这里提到的 Kubernetes 核心监控数据，其实使用的是 Kubernetes 的一个非常重要的扩展能力，叫作 Metrics Server。

Metrics Server 并不是 kube-apiserver 的一部分，而是通过 Aggregator 这种插件机制，在独立部署的情况下同 kube-apiserver 一起统一对外服务的。

而 Aggregator 模式的开启也非常简单：
如果你是使用 kubeadm 或者官方的 kube-up.sh 脚本部署 Kubernetes 集群的话，Aggregator 模式就是默认开启的；
如果是手动 DIY 搭建的话，你就需要在 kube-apiserver 的启动参数里加上如下所示的配置：
	
	--requestheader-client-ca-file=<path to aggregator CA cert>
	--requestheader-allowed-names=front-proxy-client
	--requestheader-extra-headers-prefix=X-Remote-Extra-
	--requestheader-group-headers=X-Remote-Group
	--requestheader-username-headers=X-Remote-User
	--proxy-client-cert-file=<path to aggregator proxy cert>
	--proxy-client-key-file=<path to aggregator proxy key>
而这些配置的作用，主要就是为 Aggregator 这一层设置对应的 Key 和 Cert 文件。而这些文件的生成，就需要你自己手动完成了，具体流程请参考这篇官方文档。Aggregator 功能开启之后，你只需要将 Metrics Server 的 YAML 文件部署起来，如下所示：

	$ git clone https://github.com/kubernetes-incubator/metrics-server
	$ cd metrics-server
	$ kubectl create -f deploy/1.8+/

接下来，你就会看到 metrics.k8s.io 这个 API 出现在了你的 Kubernetes API 列表当中。在理解了 Prometheus 关心的三种监控数据源，以及 Kubernetes 的核心 Metrics 之后，作为用户，你其实要做的就是将 Prometheus Operator 在 Kubernetes 集群里部署起来。然后，按照本篇文章一开始介绍的架构，把上述 Metrics 源配置起来，让 Prometheus 自己去进行采集即可。在后续的文章中，我会为你进一步剖析 Kubernetes 监控体系以及自定义 Metrics （自定义监控指标）的具体技术点。

#### Custom Metrics
Kubernetes 里的 Custom Metrics 机制，也是借助 Aggregator APIServer 扩展机制来实现的。这里的具体原理是，当你把 Custom Metrics APIServer 启动之后，Kubernetes 里就会出现一个叫作custom.metrics.k8s.io的 API。而当你访问这个 URL 时，Aggregator 就会把你的请求转发给 Custom Metrics APIServer 。而 Custom Metrics APIServer 的实现，其实就是一个 Prometheus 项目的 Adaptor。

比如，现在我们要实现一个根据指定 Pod 收到的 HTTP 请求数量来进行 Auto Scaling 的 Custom Metrics，这个 Metrics 就可以通过访问如下所示的自定义监控 URL 获取到：
	
	https://<apiserver_ip>/apis/custom-metrics.metrics.k8s.io/v1beta1/namespaces/default/pods/sample-metrics-app/http_requests 
这里的工作原理是，当你访问这个 URL 的时候，Custom Metrics APIServer 就会去 Prometheus 里查询名叫 sample-metrics-app 这个 Pod 的 http_requests 指标的值，然后按照固定的格式返回给访问者。当然，http_requests 指标的值，就需要由 Prometheus 按照我在上一篇文章中讲到的核心监控体系，从目标 Pod 上采集。这里具体的做法有很多种，最普遍的做法，就是让 Pod 里的应用本身暴露出一个 /metrics API，然后在这个 API 里返回自己收到的 HTTP 的请求的数量。所以说，接下来 HPA 只需要定时访问前面提到的自定义监控 URL，然后根据这些值计算是否要执行 Scaling 即可。

首先，我们当然是先部署 Prometheus 项目。这一步，我当然会使用 Prometheus Operator 来完成，如下所示：

	$ kubectl apply -f demos/monitoring/prometheus-operator.yaml
	clusterrole "prometheus-operator" created
	serviceaccount "prometheus-operator" created
	clusterrolebinding "prometheus-operator" created
	deployment "prometheus-operator" created
	
	$ kubectl apply -f demos/monitoring/sample-prometheus-instance.yaml
	clusterrole "prometheus" created
	serviceaccount "prometheus" created
	clusterrolebinding "prometheus" created
	prometheus "sample-metrics-prom" created
	service "sample-metrics-prom" created

第二步，我们需要把 Custom Metrics APIServer 部署起来，如下所示：
	
	$ kubectl apply -f demos/monitoring/custom-metrics.yaml
	namespace "custom-metrics" created
	serviceaccount "custom-metrics-apiserver" created
	clusterrolebinding "custom-metrics:system:auth-delegator" created
	rolebinding "custom-metrics-auth-reader" created
	clusterrole "custom-metrics-read" created
	clusterrolebinding "custom-metrics-read" created
	deployment "custom-metrics-apiserver" created
	service "api" created
	apiservice "v1beta1.custom-metrics.metrics.k8s.io" created
	clusterrole "custom-metrics-server-resources" created
	clusterrolebinding "hpa-controller-custom-metrics" created
第三步，我们需要为 Custom Metrics APIServer 创建对应的 ClusterRoleBinding，以便能够使用 curl 来直接访问 Custom Metrics 的 API：

	$ kubectl create clusterrolebinding allowall-cm --clusterrole custom-metrics-server-resources --user system:anonymous
	clusterrolebinding "allowall-cm" created
第四步，我们就可以把待监控的应用和 HPA 部署起来了，如下所示
	
	$ kubectl apply -f demos/monitoring/sample-metrics-app.yaml
	deployment "sample-metrics-app" created
	service "sample-metrics-app" created
	servicemonitor "sample-metrics-app" created
	horizontalpodautoscaler "sample-metrics-app-hpa" created
	ingress "sample-metrics-app" created

这里，我们需要关注一下 HPA 的配置，如下所示：

	kind: HorizontalPodAutoscaler
	apiVersion: autoscaling/v2beta1
	metadata:
	  name: sample-metrics-app-hpa
	spec:
	  scaleTargetRef:
	    apiVersion: apps/v1
	    kind: Deployment
	    name: sample-metrics-app
	  minReplicas: 2
	  maxReplicas: 10
	  metrics:
	  - type: Object
	    object:
	      target:
	        kind: Service
	        name: sample-metrics-app
	      metricName: http_requests
	      targetValue: 100
可以看到，HPA 的配置，就是你设置 Auto Scaling 规则的地方。比如，scaleTargetRef 字段，就指定了被监控的对象是名叫 sample-metrics-app 的 Deployment，也就是我们上面部署的被监控应用。并且，它最小的实例数目是 2，最大是 10。

在 metrics 字段，我们指定了这个 HPA 进行 Scale 的依据，是名叫 http_requests 的 Metrics。而获取这个 Metrics 的途径，则是访问名叫 sample-metrics-app 的 Service。

了这些字段里的定义， HPA 就可以向如下所示的 URL 发起请求来获取 Custom Metrics 的值了：

	https://<apiserver_ip>/apis/custom-metrics.metrics.k8s.io/v1beta1/namespaces/default/services/sample-metrics-app/http_requests

需要注意的是，上述这个 URL 对应的被监控对象，是我们的应用对应的 Service。这跟本文一开始举例用到的 Pod 对应的 Custom Metrics URL 是不一样的。当然，对于一个多实例应用来说，通过 Service 来采集 Pod 的 Custom Metrics 其实才是合理的做法。

为我们的应用增加一些访问压力,

	$ # Install hey
	$ docker run -it -v /usr/local/bin:/go/bin golang:1.8 go get github.com/rakyll/hey
	
	$ export APP_ENDPOINT=$(kubectl get svc sample-metrics-app -o template --template {{.spec.clusterIP}}); echo ${APP_ENDPOINT}
	$ hey -n 50000 -c 1000 http://${APP_ENDPOINT}
如果你去访问应用 Service 的 Custom Metircs URL，就会看到这个 URL 已经可以为你返回应用收到的 HTTP 请求数量了，如下所示：

	$ curl -sSLk https://<apiserver_ip>/apis/custom-metrics.metrics.k8s.io/v1beta1/namespaces/default/services/sample-metrics-app/http_requests
	{
	  "kind": "MetricValueList",
	  "apiVersion": "custom-metrics.metrics.k8s.io/v1beta1",
	  "metadata": {
	    "selfLink": "/apis/custom-metrics.metrics.k8s.io/v1beta1/namespaces/default/services/sample-metrics-app/http_requests"
	  },
	  "items": [
	    {
	      "describedObject": {
	        "kind": "Service",
	        "name": "sample-metrics-app",
	        "apiVersion": "/__internal"
	      },
	      "metricName": "http_requests",
	      "timestamp": "2018-11-30T20:56:34Z",
	      "value": "501484m"
	    }
	  ]
	}
这里需要注意的是，Custom Metrics API 为你返回的 Value 的格式。

在为被监控应用编写 /metrics API 的返回值时，我们其实比较容易计算的，是该 Pod 收到的 HTTP request 的总数。所以，我们这个应用的代码其实是如下所示的样子：

	  if (request.url == "/metrics") {
	    response.end("# HELP http_requests_total The amount of requests served by the server in total\n# TYPE http_requests_total counter\nhttp_requests_total " + totalrequests + "\n");
	    return;
	  }
可以看到，我们的应用在 /metrics 对应的 HTTP response 里返回的，其实是 http_requests_total 的值。这，也就是 Prometheus 收集到的值。而 Custom Metrics APIServer 在收到对 http_requests 指标的访问请求之后，它会从 Prometheus 里查询 http_requests_total 的值，然后把它折算成一个以时间为单位的请求率，最后把这个结果作为 http_requests 指标对应的值返回回去。所以说，我们在对前面的 Custom Metircs URL 进行访问时，会看到值是 501484m，这里的格式，其实就是 milli-requests，相当于是在过去两分钟内，每秒有 501 个请求。这样，应用的开发者就无需关心如何计算每秒的请求个数了。而这样的“请求率”的格式，是可以直接被 HPA 拿来使用的。这时候，如果你同时查看 Pod 的个数的话，就会看到 HPA 开始增加 Pod 的数目了。不过，在这里你可能会有一个疑问，Prometheus 项目，又是如何知道采集哪些 Pod 的 /metrics API 作为监控指标的来源呢。实际上，如果仔细观察一下我们前面创建应用的输出，你会看到有一个类型是 ServiceMonitor 的对象也被创建了出来。它的 YAML 文件如下所示：

	apiVersion: monitoring.coreos.com/v1
	kind: ServiceMonitor
	metadata:
	  name: sample-metrics-app
	  labels:
	    service-monitor: sample-metrics-app
	spec:
	  selector:
	    matchLabels:
	      app: sample-metrics-app
	  endpoints:
	  - port: web

这个 ServiceMonitor 对象，正是 Prometheus Operator 项目用来指定被监控 Pod 的一个配置文件。可以看到，我其实是通过 Label Selector 为 Prometheus 来指定被监控应用的。

***
### 让日志无处可逃：容器日志收集与管理
1. 应用把日志输出到 stdout 和 stderr 之后,容器项目在默认情况下就会把这些日志输出到宿主机上的一个 JSON 文件里。这样，你通过 kubectl logs 命令就可以看到这些容器的日志了。在 Node 上部署 logging agent，将日志文件转发到后端存储里保存起来。不难看到，这里的核心就在于 logging agent ，它一般都会以 DaemonSet 的方式运行在节点上，然后将宿主机上的容器日志目录挂载进去，最后由 logging-agent 把日志转发出去。举个例子，我们可以通过 Fluentd 项目作为宿主机上的 logging-agent，然后把日志转发到远端的 ElasticSearch 里保存起来供将来进行检索。具体的操作过程，你可以通过阅读这篇文档来了解。另外，在很多 Kubernetes 的部署里，会自动为你启用 logrotate，在日志文件超过 10MB 的时候自动对日志文件进行 rotate 操作。可以看到，在 Node 上部署 logging agent 最大的优点，在于一个节点只需要部署一个 agent，并且不会对应用和 Pod 有任何侵入性。所以，这个方案，在社区里是最常用的一种。但是也不难看到，这种方案的不足之处就在于，它要求应用输出的日志，都必须是直接输出到容器的 stdout 和 stderr 里。
2. sidecar 容器
通过一个 sidecar 容器，直接把应用的日志文件发送到远程存储里面去,ogging agent，放在了应用 Pod 里。这种方案的架构如下所示：
![log](k8s/8.jpg)

的应用还可以直接把日志输出到固定的文件里而不是 stdout，你的 logging-agent 还可以使用 fluentd，logstash,后端存储还可以是 ElasticSearch。只不过， fluentd 的输入源，变成了应用的日志文件。一般来说，我们会把 fluentd 的输入源配置保存在一个 ConfigMap 里，如下所示：

	apiVersion: v1
	kind: ConfigMap
	metadata:
	  name: fluentd-config
	data:
	  fluentd.conf: |
	    <source>
	      type tail
	      format none
	      path /var/log/1.log
	      pos_file /var/log/1.log.pos
	      tag count.format1
	    </source>
	    
	    <source>
	      type tail
	      format none
	      path /var/log/2.log
	      pos_file /var/log/2.log.pos
	      tag count.format2
	    </source>
	    
	    <match **>
	      type google_cloud
	    </match>

然后，我们在应用 Pod 的定义里，就可以声明一个 Fluentd 容器作为 sidecar，专门负责将应用生成的 1.log 和 2.log 转发到 ElasticSearch 当中。这个配置，如下所示：

	apiVersion: v1
	kind: Pod
	metadata:
	  name: counter
	spec:
	  containers:
	  - name: count
	    image: busybox
	    args:
	    - /bin/sh
	    - -c
	    - >
	      i=0;
	      while true;
	      do
	        echo "$i: $(date)" >> /var/log/1.log;
	        echo "$(date) INFO $i" >> /var/log/2.log;
	        i=$((i+1));
	        sleep 1;
	      done
	    volumeMounts:
	    - name: varlog
	      mountPath: /var/log
	  - name: count-agent
	    image: k8s.gcr.io/fluentd-gcp:1.30
	    env:
	    - name: FLUENTD_ARGS
	      value: -c /etc/fluentd-config/fluentd.conf
	    volumeMounts:
	    - name: varlog
	      mountPath: /var/log
	    - name: config-volume
	      mountPath: /etc/fluentd-config
	  volumes:
	  - name: varlog
	    emptyDir: {}
	  - name: config-volume
	    configMap:
	      name: fluentd-config

可以看到，这个 Fluentd 容器使用的输入源，就是通过引用我们前面编写的 ConfigMap 来指定的。这里我用到了 Projected Volume 来把 ConfigMap 挂载到 Pod 里。如果你对这个用法不熟悉的话，可以再回顾下第 15 篇文章《 深入解析 Pod 对象（二）：使用进阶》中的相关内容。

需要注意的是，这种方案虽然部署简单，并且对宿主机非常友好，但是这个 sidecar 容器很可能会消耗较多的资源，甚至拖垮应用容器。并且，由于日志还是没有输出到 stdout 上，所以你通过 kubectl logs 是看不到任何日志输出的。

还有一种方式就是在编写应用的时候，就直接指定好日志的存储后端.